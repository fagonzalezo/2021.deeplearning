
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>05 - SEQUENCE MODELS &#8212; Fundamentos de Deep Learning</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.0 Crossvalidation in time series" href="U5.00%20-%20Intro%20time%20series.html" />
    <link rel="prev" title="LAB 4.4 - Semantic segmentation" href="U4%20LAB%2004%20-%20Semantic%20segmentation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/fudea.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Fundamentos de Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="M00.html">
   Información 20212 - UdeA
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M01.html">
   01 - INTRODUCTION
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="U1.01%20-%20DL%20Overview.html">
     1.1 - DL Overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1.02%20-%20Modelos%20derivados%20de%20los%20datos.html">
     1.2 - Models derived from data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1.03%20-%20Como%20se%20disena%20un%20algoritmo%20de%20Machine%20Learning.html">
     1.3 - ML algorithm design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1%20LAB%2001%20-%20WARMUP.html">
     LAB 01.01 - WARM UP
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M02.html">
   02 - NEURAL NETWORKS
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.01%20-%20The%20Perceptron.html">
     2.1 - The Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.02%20-%20The%20Multilayer%20Perceptron.html">
     2.2 - The Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.03%20-%20Overfitting%20and%20regularization.html">
     2.3 - Overfitting and regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.04%20-%20Loss%20functions.html">
     2.4 - Loss functions in Tensorflow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.05%20-%20Network%20Architectures%20-%20Autoencoders.html">
     2.5 - Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.06%20-%20Network%20Architectures%20-%20Multimodal%20information.html">
     2.6 - Multimodal architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.07%20-%20Vanishing%20gradients.html">
     2.7 - Vanishing gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.08%20-%20Weights%20initialization.html">
     2.8 - Weights initialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2001%20-%20Customized%20loss%20functions%20and%20regularization.html">
     LAB 2.1 - Customized loss function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2002%20-%20Autoencoders.html">
     LAB 2.2 - Sparse Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2003%20-%20Pairwise%20image%20classification.html">
     LAB 2.3 - Pairwise classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2004%20-%20Model%20instrumentation%20and%20monitoring.html">
     LAB 2.4 - Model instrumentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M03.html">
   03 - TENSORFLOW CORE
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.01%20-%20Simbolic%20computing%20for%20ML.html">
     3.1 - Symbolic computing for ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.02%20-%20TF%20for%20symbolic%20computing.html">
     3.2 - TF symbolic engine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.03%20-%20Using%20tf.function.html">
     3.3 - Using
     <code class="docutils literal notranslate">
      <span class="pre">
       tf.function
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.04%20-%20Batch%20Normalization.html">
     3.4 - Batch normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3%20LAB%2001%20-%20Tensorflow%20model%20subclassing.html">
     LAB 3.1 - TF model subclassing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3%20LAB%2002%20-%20Low%20level%20Tensorflow.html">
     LAB 3.2 - Low level
     <code class="docutils literal notranslate">
      <span class="pre">
       tensorflow
      </span>
     </code>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="M04.html">
   04 - CONVOLUTIONAL NETWORKS
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.01%20-%20Convolutions.html">
     4.1 - Convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.02%20-%20Convolutional%20Neural%20Networks.html">
     4.2 - CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.03%20-%20Dropout%2C%20pooling.html">
     4.3 - Dropout, pooling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.04%20-%20CNN%20Architectures.html">
     4.4 - CNN Architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.05%20-%20Transfer%20learning.html">
     4.5 - Transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.06%20-%20Object%20Detection.html">
     4.6 - Object detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.07%20-%20Transposed%20convolutions.html">
     4.7 - Transposed convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.08%20-%20UNet%20image%20segmentation.html">
     <strong>
      4.8
     </strong>
     - UNet Image segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.09%20-%20Atrous%20convolutions.html">
     4.9 - Atrous convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2001%20-%20Convolutions.html">
     LAB 4.1 - Convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2002%20-%20Transfer%20Learning.html">
     LAB 4.2 - Transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2003%20-%20Object%20Detection.html">
     LAB 4.3 - Object detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2004%20-%20Semantic%20segmentation.html">
     LAB 4.4 - Semantic segmentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   05 - SEQUENCE MODELS
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.00%20-%20Intro%20time%20series.html">
     5.0 Crossvalidation in time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.01%20-%20Recurrent%20Neural%20Networks.html">
     5.1 Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.02%20-%20Long%20Short%20Term%20Memory%20RNN.html">
     5.2 LSTM and GRU
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.03%20-%20Truncated%20BPTT.html">
     5.3 Truncated BPTT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.04%20-%20Basic%20concepts%20of%20text%20processing.html">
     5.4 Text processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.05%20-%20Sequences%20generation%20using%20LSTM.html">
     5.5 Sequences generation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.06%20-%20Bidirectional%20RNNs%20-%20Attention%20Model.html">
     5.6 Bidirectional RNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.07%20-%20ELMo%20-%20NER.html">
     5.7 ELMo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.08%20-%20Self-Attention%20-%20Transformer%20-%20BERT.html">
     5.8 Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.09%20-%20CNN-LSTM%20architectures.html">
     5.9  CNN-LSTM architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5%20LAB%2001%20-%20Multivariate%20time%20series%20prediction.html">
     LAB 5.1 - Time series prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5%20LAB%2002%20-%20Padding%2C%20Masking%20-%20Sentiment%20Analysis.html">
     LAB 5.2 - Padding - Masking
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5%20LAB%2003%20-%20Sentiment%20Analysis%20using%20BERT.html">
     LAB 5.3 - Transformer - BERT
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/M05.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   Introducción
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#redes-neuronales-recurrentes">
   Redes Neuronales Recurrentes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#arquitecturas-recurrentes">
   Arquitecturas recurrentes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laboratorio-1">
   LABORATORIO 1
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#procesamiento-de-texto">
   Procesamiento de texto
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laboratorio-2">
   LABORATORIO 2
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#redes-neuronales-recurrentes-bidirecionales">
   Redes Neuronales Recurrentes Bidirecionales
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#el-modelo-transformer">
   El modelo Transformer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laboratorio-3">
   LABORATORIO 3
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#arquitecturas-cnn-lstm">
   Arquitecturas CNN-LSTM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laboratorio">
   LABORATORIO
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="sequence-models">
<h1>05 - SEQUENCE MODELS<a class="headerlink" href="#sequence-models" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduccion">
<h2>Introducción<a class="headerlink" href="#introduccion" title="Permalink to this headline">¶</a></h2>
<p><strong>01 - Validación cruzada en problemas de series de tiempo</strong>: <a class="reference external" href="https://youtu.be/w8xfaSksicQ">Video 11mins</a><br/> Describimos las particularidades del proceso de validación en problemas de series de tiempo, una de las aplicaciones del procesamiento de datos secuenciales.</p>
<p><strong>02 - Tareas de analítica de secuencias</strong>: <a class="reference external" href="https://youtu.be/xTxLaCqUbBk">Video 13mins</a><br/>Presentamos de manera general el principio de funcionamiento de una red recurrente, los tipos de aplicaciones que se presentan en el análisis de de secuencias y las configuraciones de redes más comunes.</p>
</div>
<div class="section" id="redes-neuronales-recurrentes">
<h2>Redes Neuronales Recurrentes<a class="headerlink" href="#redes-neuronales-recurrentes" title="Permalink to this headline">¶</a></h2>
<p><strong>03 - Introducción a las Redes Neuronales Recurrentes</strong>: <a class="reference external" href="https://youtu.be/n5ropbj3lno">Video 13mins</a><br/>  Describimos los principios de funcionamiento de las RNN y la analizamos como una red densa de muchas capas.</p>
<p><strong>04 - Algoritmo de Backpropagation Through Time</strong>: <a class="reference external" href="https://youtu.be/UiUSgNIvev8">Video 11mins</a><br/>  Analizamos los principios de funcionamiento del algoritmo de entrenamiento de las RNN y sus implicaciones en términos de computacionales.</p>
<p><strong>05 - Implementación de RNN en TensorFlow</strong>: <a class="reference external" href="https://youtu.be/YLeoRmmYmq4">Video 17mins</a><br/> Usamos un problema de series de tiempo para implementar RNNs de una y dos capas ocultas en TensorFlow. Describimos los elementos básicos de una arquitectura Codificador-Decodificador.</p>
</div>
<div class="section" id="arquitecturas-recurrentes">
<h2>Arquitecturas recurrentes<a class="headerlink" href="#arquitecturas-recurrentes" title="Permalink to this headline">¶</a></h2>
<p><strong>06 - Implementación de Arquitecturas de RNN para problemas seq-to-seq</strong>: <a class="reference external" href="https://youtu.be/jitQc7YusUA">Video 19mins</a><br/>  Usamos un problema de series de tiempo en el que se desean predecir varios tiempos hacia adelante, para describir e implementar tres metodologías/arquitecturas para la solución de problemas donde tanto la entrada como la salida son secuencias.</p>
<p><strong>07 - Long Short Term Memory RNN</strong>: <a class="reference external" href="https://youtu.be/jVei1bWFXMc">Video 22mins</a><br/> Presentamos los principios de funcionamiento de las redes recurrentes de tipo LSTM y GRM, así como su implementación en TensorFlow.</p>
<p><strong>08 - Truncated BPTT</strong>: <a class="reference external" href="https://youtu.be/oSVbUKl2nYQ">Video 24mins</a> <br/> Presentamos una variante del algoritmo de Propagación hacia atrás en el tiempo que permite realizar actualizaciones de los parámetros de la red, a partir de propagaciones parciales de una secuencia y cómo se puede realizar su implementación utilizando el framework de tensorflow.</p>
</div>
<div class="section" id="laboratorio-1">
<h2>LABORATORIO 1<a class="headerlink" href="#laboratorio-1" title="Permalink to this headline">¶</a></h2>
<p><strong>LAB 1 - Multivariate Time-series prediction</strong>: <a class="reference external" href="https://youtu.be/oK4pDy7Q1MQ">Video 6mins</a> <br/>En este laboratorio debes diseñar diferentes arquitecturas de redes RNN para predecir un sólo tiempo hacia adelante y varios tiempos hacia adelante en un problema de series de tiempo multivariado.</p>
</div>
<div class="section" id="procesamiento-de-texto">
<h2>Procesamiento de texto<a class="headerlink" href="#procesamiento-de-texto" title="Permalink to this headline">¶</a></h2>
<p><strong>09 - Introducción al procesamiento de texto</strong>: <a class="reference external" href="https://youtu.be/IwEPJQEX-lc">Video 17mins</a> <br/> Describimos las principales etapas de preprocesamiento necesarias para el uso de modelos de Machine Learning en tareas de procesamiento de lenguaje natural.</p>
<p><strong>10 - Word embeddings</strong>: <a class="reference external" href="https://youtu.be/lqXdZOq9U_0">Video 25mins</a> <br/> Peresentamos las razones que inspiraron la creación de técnicas de embebimiento de palabras y discutimos diversas variantes, sus principios de funcionamiento y la estrategia de transfer learning que permite usar word embeddings pre-entrenados dentro de modelos de Deep Learning en tensorflow.</p>
<p><strong>11 - Generación de secuencias usando RNNs</strong>: <a class="reference external" href="https://youtu.be/VSswvuwTz-g">Video 9mins</a> <br/> Describimos el procedimiento que puede ser empleado para generar secuencias artificiales a partir de redes neuronales recurrentes pre-entrenadas.</p>
</div>
<div class="section" id="laboratorio-2">
<h2>LABORATORIO 2<a class="headerlink" href="#laboratorio-2" title="Permalink to this headline">¶</a></h2>
<p><strong>LAB 2 - Sentiment analysis in text</strong>: <a class="reference external" href="https://youtu.be/tUkhHJTvE-o">Video 10mins</a> <br/>En este laboratorio debes diseñar diferentes arquitecturas de DL y estrategias de transfer learning para clasificar tweets como positivos o negativos.</p>
</div>
<div class="section" id="redes-neuronales-recurrentes-bidirecionales">
<h2>Redes Neuronales Recurrentes Bidirecionales<a class="headerlink" href="#redes-neuronales-recurrentes-bidirecionales" title="Permalink to this headline">¶</a></h2>
<p><strong>12 - Redes RNN Bidirecionales</strong>: <a class="reference external" href="https://youtu.be/GneNfVlNq8E">Video 16mins</a> <br/> Presentamos las limitaciones que tienen las redes neuronales recurrentes para procesar secuencias en las que la predicción para una posición de la secuencia, depende no sólo de las observaciones anteriores de la secuencia sino también de observaciones futuras y cómo las redes neuronales bidireccionales resuelven ese problema.</p>
<p><strong>13 - Arquitectura Encoder-Decoder con mecanismo de atención</strong>: <a class="reference external" href="https://youtu.be/XsgF5bFWcew">Video 17mins</a> <br/> Describimos una arquitectura particular de red recurrente de tipo codificador-decodificador, en la que se dota a la capa del decodificador de la cpacidad para seleccionar qué información de la secuencia de entrada es relevante para realizar cada una de las predicciones en la secuencia de salida.</p>
<p><strong>14 - ELMo: Embeddings from Language Models</strong>: <a class="reference external" href="https://youtu.be/GC9zr2wPtZo">Video 23mins</a> <br/> Describimos una arquitectura de red conocida como ELMo que permite obtener vectores de embebimiento a partir de un procesamiento de secuencias a nivel de caracter, esto permite que el vector que representa a una palabra pueda depender del contexto y no sea siempre estático.</p>
</div>
<div class="section" id="el-modelo-transformer">
<h2>El modelo Transformer<a class="headerlink" href="#el-modelo-transformer" title="Permalink to this headline">¶</a></h2>
<p><strong>15 - Mecanismo de auto-atención</strong>: <a class="reference external" href="https://youtu.be/p727fQCrw9c">Video 24mins</a> <br/> Describimos el modelo conocido como Transformer y su principal principio de funcionamiento: el mecanismo de auto-atención.</p>
<p><strong>16 - Modelo BERT</strong>: <a class="reference external" href="https://youtu.be/XTtcdIXskvY">Video 5mins</a> <br/> Describimos el modelo BERT (Bidirectional Encoder Representations from Transformers) muy usado como modelo base para diferentes aplicaciones de NLP entre otras.</p>
</div>
<div class="section" id="laboratorio-3">
<h2>LABORATORIO 3<a class="headerlink" href="#laboratorio-3" title="Permalink to this headline">¶</a></h2>
<p><strong>LAB 3 - Sentiment analysis with a Self-Attention Layer</strong>: <a class="reference external" href="https://youtu.be/gvgjpkCAJcs">Video 15mins</a> <br/>En este laboratorio debes diseñar diferentes arquitecturas de DL basadas en el modelo de Transformer para clasificar tweets como positivos o negativos.</p>
</div>
<div class="section" id="arquitecturas-cnn-lstm">
<h2>Arquitecturas CNN-LSTM<a class="headerlink" href="#arquitecturas-cnn-lstm" title="Permalink to this headline">¶</a></h2>
<p><strong>17 - Arquitecturas CNN-LSTM y ConvLSTM</strong>: <a class="reference external" href="https://youtu.be/deVW91RR_lQ">Video 22mins</a> <br/> Presentamos las arquitecturas de redes neuronales profundas que pueden ser usadas para procesar secuencias de matrices u objetos 3D. Este tipo de arquitecturas permiten resolver tareas de ML sobre videos.</p>
</div>
<div class="section" id="laboratorio">
<h2>LABORATORIO<a class="headerlink" href="#laboratorio" title="Permalink to this headline">¶</a></h2>
<p><strong>NON-REQUIRED LAB</strong></p>
<p><strong>LAB 4 - Video classification</strong>: <br/>En este laboratorio debes diseñar arquitecturas de DL para clasificar videos de acuerdo con la acción que las personas están realizando en ellos. Este laboratorio no puede ser ejecutado en el colab, ya que la memorúa RAM disponible no es suficiente para cargar la BD que consiste de dos clases, cada una de 24 videos de 30 frames cada uno. Para quienes quieran desarrollar el ejercicio de construir una arquitectura para clasificación de videos con base en una arquitectura CNN pre-entrenada, el notebook puede ser consultado dentro del respositorio en el siguiente <a class="reference external" href="https://github.com/rramosp/2021.deeplearning/blob/main/content/U5%20LAB%2004%20-%20Video%20Classification.ipynb">enlace</a> y debe ser ejecutado en local para lo cual deben tener instalada toda la suite necesaria de librerías y paquetes.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="U4%20LAB%2004%20-%20Semantic%20segmentation.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">LAB 4.4 - Semantic segmentation</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="U5.00%20-%20Intro%20time%20series.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">5.0 Crossvalidation in time series</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Raúl Ramos, Julián Arias / Universidad de Antioquia<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-43235448-3', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>