{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LAB 5.2 - Padding - Masking"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["!wget -nc --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2021.deeplearning/main/content/init.py\n", "import init; init.init(force_download=False); init.get_weblink() "]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["from local.lib.rlxmoocapi import submit, session\n", "import inspect\n", "student = session.Session(init.endpoint).login( course_id=init.course_id, lab_id=\"L05.02\" )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "if 'google.colab' in sys.modules:\n", "    print (\"setting tensorflow version in colab\")\n", "    %tensorflow_version 2.x\n", "import tensorflow as tf\n", "tf.__version__"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["#Basic required libraries\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### The aim of this lab is to build a system for sentiment analysis on a dataset of tweets."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The data consist on passenger's reviews of U.S. airlines: https://www.kaggle.com/crowdflower/twitter-airline-sentiment "]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["data = pd.read_csv('local/data/Tweets.csv')\n", "# Keeping only the neccessary columns\n", "data = data[['text','airline_sentiment']]"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["data"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["import re\n", "pd.options.mode.chained_assignment = None  # default='warn'\n", "#Remove neutral class\n", "data = data[data.airline_sentiment != \"neutral\"]\n", "\n", "#text normalization\n", "data['text'] = data['text'].apply(lambda x: x.lower())\n", "data['text'] = data['text'].apply((lambda x:re.sub('@[^\\s]+','',x)))#remove the name of the airline\n", "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n", "Np = np.sum(data['airline_sentiment'].values == 'positive')\n", "Nn = np.sum(data['airline_sentiment'].values == 'negative')\n", "print(f'Number of positive samples={Np}')\n", "print(f'Number of negative samples={Nn}')\n", "for idx,row in data.iterrows():\n", "    row[0] = row[0].replace('rt',' ')"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["data"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import nltk\n", "nltk.download('punkt')\n", "nltk.download('stopwords')"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Removing Stop Words\n", "from nltk.corpus import stopwords\n", "all_sentences = data['text'].values\n", "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n", "stop_words = stopwords.words('english')\n", "stop_words.append('')\n", "\n", "for i in range(len(all_words)):  \n", "    all_words[i] = [w for w in all_words[i] if (w not in stop_words) and (not w.isdigit())]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 1\n", "\n", "all_words is a list with all the tweets that are going to be used to train the model. Create a function 'get_preprocessed_seq' that build and apply a Tokenizer to the a list like all_words. Tokenizer must define a dictionary of 2000 words (remeber that position 0 is reserved). Once the sentences are tokenized, take into account that the length of every tweet is different so before they can be passed to the training step, the tweets must be **padded** in order to provide them with equal length. The function 'get_preprocessed_seq' must return the Tokenizer object and the padded dataset.\n", "\n", "Review the [padding function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) in the preprocessing module of keras. "]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["def get_preprocessed_seq(text_list):\n", "    ...\n", "    return tokenizer, Xdata "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Submit your solution:**"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='L05.02', task_id='T1');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 2\n", "\n", "The previous step add 0's to some tweets in order to set them with the same length. Now it is necessary to define a model that be able to discard those 0's. Review the masking layer and masking option of the embedding layer of keras. Take a look to the TensorFlow [documentation](https://www.tensorflow.org/guide/keras/masking_and_padding).\n", "\n", "Create a function to define a RNN architecture for the sentiment analysis problem. Use the Embedding layer and its masking option to discard the 0's added during padding step. The architecture must consist of a RNN layer with a 'cells_number' of neurons, a dense hidden layer of 10 neurons and the output layer. Include a Dropout layer in between the dense layers with a drop rate of 0.3 . The type of RNN layer must be defined through a 'layer_type' parameter; the embedding dimension must also be set as an input parameter. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Recurrent_Model(cells_number = 10, layer_type='SimpleRNN',Embeb_dim=32):#Options for layer_type: 'SimpleRNN', 'LSTM', 'GRU'\n", "    model = ...\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Submit your solution:**"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='L05.02', task_id='T2');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's test the model: \n", "\n", "**Warning**: Run this part only if you have already passed Tasks 1 and 2."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import recall_score, accuracy_score\n", "from sklearn.utils.class_weight import compute_class_weight\n", "\n", "tokenizer, NewX = get_preprocessed_seq(all_words)\n", "\n", "y = data['airline_sentiment'].values\n", "Encoder = LabelEncoder()\n", "Y = Encoder.fit_transform(y)\n", "#Y = Encoder.transform(y)\n", "\n", "\n", "X_tr, X_te, y_tr, y_te = train_test_split(NewX, Y, test_size=0.2, random_state=2018)"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["def especi_score(y_te,y_pred):\n", "    Ns = np.sum(y_te == 0)\n", "    return np.sum(y_te[y_te == 0] == y_pred[y_te == 0])/Ns"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["sensitivity = np.zeros((3,3))\n", "especificity = np.zeros((3,3))\n", "accuracy = np.zeros((3,3))\n", "for i, embed_dim in enumerate([32,64,128]):\n", "    for j,cells in enumerate([32,64,128]):\n", "        model = Recurrent_Model(cells_number= cells, layer_type='LSTM', Embeb_dim=embed_dim)\n", "        #opt = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n", "        model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n", "        model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n", "        y_pred = np.round(model.predict(X_te))\n", "        sensitivity[i,j] = recall_score(y_te,y_pred)\n", "        accuracy[i,j] = accuracy_score(y_te,y_pred)\n", "        especificity[i,j] = especi_score(y_te,y_pred.flatten())"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["from local.lib.DataPreparationRNN import Plot_sentiment_performance\n", "Plot_sentiment_performance(sensitivity,accuracy,especificity)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 3\n", "\n", "Create a similar architecture to that of task 2, but in this case use global vectors (GloVe) from gensim library to set the Embedding weights. Set the Embedding layer as non trainable. If there is any missing word in the pre-trained GloVes, you can use the token 'unk' instead.\n", "\n", "**Note**: Take care on the tokenization of the words. Keras tokenizer does not assign the zero value to any word because of padding purposes. Make sure that the order of the vectors in the GloVe embedding matrix corresponds with the indexes in the dictionary. "]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["#Run this cell to get the pre-trained word embedding vectors\n", "import gensim.downloader\n", "glove_vectors = gensim.downloader.load('glove-twitter-50')"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["#Run this cell to provide the grader with the word embedding vectors\n", "import pickle\n", "with open('GloVe.pkl', 'wb') as output:\n", "    pickle.dump(glove_vectors, output, pickle.HIGHEST_PROTOCOL)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Recurrent_Model_TF(tokenizer,glove_vectors,cells_number = 10, layer_type='SimpleRNN'):#Options for layer_type: 'SimpleRNN', 'LSTM', 'GRU'\n", "    model = ...\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Submit your solution:**"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='L05.02', task_id='T3');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's test the model: \n", "\n", "**Warning**: Run this part only if you have already passed Tasks 2 and 3."]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["sensitivity = np.zeros((3))\n", "especificity = np.zeros((3))\n", "accuracy = np.zeros((3))\n", "for j,cells in enumerate([32,64,128]):\n", "    model = Recurrent_Model_TF(tokenizer, glove_vectors, cells_number = cells, layer_type='LSTM')\n", "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n", "    model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n", "    y_pred = np.round(model.predict(X_te))\n", "    sensitivity = recall_score(y_te,y_pred)\n", "    accuracy[j] = accuracy_score(y_te,y_pred)\n", "    especificity[j] = especi_score(y_te,y_pred.flatten())"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(5,3))\n", "X = np.arange(3)\n", "ax.bar(X + 0.00, accuracy, color = 'b', width = 0.25)\n", "ax.bar(X + 0.25, sensitivity, color = 'g', width = 0.25)\n", "ax.bar(X + 0.50, especificity, color = 'r', width = 0.25)\n", "ax.set_xticks([0.25, 1.25, 2.25])\n", "ax.set_xticklabels(['32','64', '128'])\n", "ax.set_title('Performance')\n", "ax.set_xlabel('Number of cells')\n", "ax.legend(labels=['accuracy','sensitivity','especificity'],bbox_to_anchor=(1.1, 1.05))\n", "print('Best accuracy= {}'.format(np.max(accuracy)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 4\n", "\n", "Create a function similar to that of task 3, but use a Conv1D layer instead of the LSTM one. The number of filters (kernels) must be an input parameter and define the kernels in order for them to use trigrams. Use the GloVe embedding weights from the former task. To complete the architecture and shape correctly the tensors, you must use a GlobalMaxPooling1D layer after the CNN layer."]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["def Con1D_Model_TF(tokenizer, glove_vectors, filters = 10):\n", "    ...\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Submit your solution:**"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["student.submit_task(namespace=globals(), course_id=init.course_id, lab_id='L05.02', task_id='T4');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's test the model: \n", "\n", "**Warning**: Run this part only if you have already passed Tasks 3 and 4."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sensitivity = np.zeros((3))\n", "especificity = np.zeros((3))\n", "accuracy = np.zeros((3))\n", "for j,cells in enumerate([6,12,24]):\n", "    model = Con1D_Model_TF(tokenizer, glove_vectors, filters = cells)\n", "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n", "    model.fit(X_tr, y_tr, validation_split=0.1,batch_size=32, epochs=10, verbose=1)\n", "    y_pred = np.round(model.predict(X_te))\n", "    sensitivity = recall_score(y_te,y_pred)\n", "    accuracy[j] = accuracy_score(y_te,y_pred)\n", "    especificity[j] = especi_score(y_te,y_pred.flatten())"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(5,3))\n", "X = np.arange(3)\n", "ax.bar(X + 0.00, accuracy, color = 'b', width = 0.25)\n", "ax.bar(X + 0.25, sensitivity, color = 'g', width = 0.25)\n", "ax.bar(X + 0.50, especificity, color = 'r', width = 0.25)\n", "ax.set_xticks([0.25, 1.25, 2.25])\n", "ax.set_xticklabels(['6','12', '24'])\n", "ax.set_title('Performance')\n", "ax.set_xlabel('Number of filters')\n", "ax.legend(labels=['accuracy','sensitivity','especificity'],bbox_to_anchor=(1.1, 1.05))\n", "print('Best accuracy= {}'.format(np.max(accuracy)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 2}