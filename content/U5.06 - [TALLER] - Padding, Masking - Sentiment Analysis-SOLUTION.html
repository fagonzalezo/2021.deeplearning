
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The aim of this lab is to build a system for sentiment analysis on a dataset of tweets. &#8212; Fundamentos de Deep Learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/fudea.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Fundamentos de Deep Learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="M00.html">
   Informaci√≥n 20211 - UdeA
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="M01.html">
   01 - INTRODUCTION
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="U1.01%20-%20DL%20Overview.html">
     1.1 - DL Overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1.02%20-%20Modelos%20derivados%20de%20los%20datos.html">
     1.2 - Models derived from data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1.03%20-%20Como%20se%20disena%20un%20algoritmo%20de%20Machine%20Learning.html">
     1.3 - ML algorithm design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U1%20LAB%2001%20-%20WARMUP.html">
     LAB 01.01 - WARM UP
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="M02.html">
   02 - NEURAL NETWORKS
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="U2.01%20-%20The%20Perceptron.html">
     2.1 - The Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.02%20-%20The%20Multilayer%20Perceptron.html">
     2.2 - The Multilayer Perceptron
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.03%20-%20Overfitting%20and%20regularization.html">
     2.3 - Overfitting and regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.04%20-%20Loss%20functions.html">
     2.4 - Loss functions in Tensorflow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.05%20-%20Network%20Architectures%20-%20Autoencoders.html">
     2.5 - Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.06%20-%20Network%20Architectures%20-%20Multimodal%20information.html">
     2.6 - Multimodal architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.07%20-%20Vanishing%20gradients.html">
     2.7 - Vanishing gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2.08%20-%20Weights%20initialization.html">
     2.8 - Weights initialization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2001%20-%20Customized%20loss%20functions%20and%20regularization.html">
     LAB 2.1 - Customized loss function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2002%20-%20Autoencoders.html">
     LAB 2.2 - Sparse Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2003%20-%20Pairwise%20image%20classification.html">
     LAB 2.3 - Pairwise classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U2%20LAB%2004%20-%20Model%20instrumentation%20and%20monitoring.html">
     LAB 2.4 - Model instrumentation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="M03.html">
   03 - TENSORFLOW CORE
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="U3.01%20-%20Simbolic%20computing%20for%20ML.html">
     3.1 - Symbolic computing for ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.02%20-%20TF%20for%20symbolic%20computing.html">
     3.2 - TF symbolic engine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.03%20-%20Using%20tf.function.html">
     3.3 - Using
     <code class="docutils literal notranslate">
      <span class="pre">
       tf.function
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3.04%20-%20Batch%20Normalization.html">
     3.4 - Batch normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3%20LAB%2001%20-%20Tensorflow%20model%20subclassing.html">
     LAB 3.1 - TF model subclassing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U3%20LAB%2002%20-%20Low%20level%20Tensorflow.html">
     LAB 3.2 - Low level
     <code class="docutils literal notranslate">
      <span class="pre">
       tensorflow
      </span>
     </code>
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="M04.html">
   04 - CONVOLUTIONAL NETWORKS
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="U4.01%20-%20Convolutions.html">
     4.1 - Convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.02%20-%20Convolutional%20Neural%20Networks.html">
     4.2 - CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.03%20-%20Dropout%2C%20pooling.html">
     4.3 - Dropout, pooling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.04%20-%20CNN%20Architectures.html">
     4.4 - CNN Architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.05%20-%20Transfer%20learning.html">
     4.5 - Transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.06%20-%20Object%20Detection.html">
     4.6 - Object detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.07%20-%20Transposed%20convolutions.html">
     4.7 - Transposed convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4.08%20-%20Image%20segmentation.html">
     4.8 - Image segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2001%20-%20Convolutions.html">
     LAB 4.1 - Convolutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2002%20-%20Transfer%20Learning.html">
     LAB 4.2 - Transfer learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U4%20LAB%2003%20-%20Object%20Detection.html">
     LAB 4.3 - Object detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="M05.html">
   05 - SEQUENCE MODELS
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="U5.00%20-%20Intro%20time%20series.html">
     5.0 Crossvalidation in time series
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.01%20-%20Recurrent%20Neural%20Networks.html">
     5.1 Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.02%20-%20Long%20Short%20Term%20Memory%20RNN.html">
     5.2 LSTM and GRU
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.03%20-%20Truncated%20BPTT.html">
     5.3 Truncated BPTT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.04%20-%20Basic%20concepts%20of%20text%20processing.html">
     5.4 Text processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.05%20-%20Sequences%20generation%20using%20LSTM.html">
     5.5 Sequences generation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.06%20-%20Bidirectional%20RNNs%20-%20Attention%20Model.html">
     5.6 Bidirectional RNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.07%20-%20Self-Attention%20-%20Transformer%20-%20BERT.html">
     5.7 Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="U5.08%20-%20CNN-LSTM%20architectures.html">
     5.8  CNN-LSTM architectures
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/rramosp/2021.deeplearning/blob/master/content/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   The aim of this lab is to build a system for sentiment analysis on a dataset of tweets.
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-1">
   Exercise 1
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-2">
   Exercise 2
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-3">
   Exercise 3
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prueba-entrenando-glove">
     Prueba entrenando GloVe
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cargando-glove-pre-entrenados">
     Cargando GloVe pre-entrenados
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-4">
   Exercise 4
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="the-aim-of-this-lab-is-to-build-a-system-for-sentiment-analysis-on-a-dataset-of-tweets">
<h1>The aim of this lab is to build a system for sentiment analysis on a dataset of tweets.<a class="headerlink" href="#the-aim-of-this-lab-is-to-build-a-system-for-sentiment-analysis-on-a-dataset-of-tweets" title="Permalink to this headline">¬∂</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>wget --no-cache -O init.py -q https://raw.githubusercontent.com/rramosp/2020.deeplearning/master/init.py
<span class="kn">from</span> <span class="nn">init</span> <span class="kn">import</span> <span class="n">init</span><span class="p">;</span> <span class="n">init</span><span class="p">(</span><span class="n">force_download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;setting tensorflow version in colab&quot;</span><span class="p">)</span>
    <span class="o">%</span><span class="k">tensorflow_version</span> 2.x
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> <span class="c1"># linear algebra</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> <span class="c1"># data processing, CSV file I/O (e.g. pd.read_csv)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>The data consist on passenger‚Äôs reviews of U.S. airlines: https://www.kaggle.com/crowdflower/twitter-airline-sentiment</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2020.deeplearning/local/data/Tweets.csv&#39;</span><span class="p">)</span>
<span class="c1"># Keeping only the neccessary columns</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span><span class="s1">&#39;airline_sentiment&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>airline_sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>@VirginAmerica What @dhepburn said.</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>1</th>
      <td>@VirginAmerica plus you've added commercials t...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>2</th>
      <td>@VirginAmerica I didn't today... Must mean I n...</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>3</th>
      <td>@VirginAmerica it's really aggressive to blast...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>@VirginAmerica and it's a really big bad thing...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>14635</th>
      <td>@AmericanAir thank you we got on a different f...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>14636</th>
      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14637</th>
      <td>@AmericanAir Please bring American Airlines to...</td>
      <td>neutral</td>
    </tr>
    <tr>
      <th>14638</th>
      <td>@AmericanAir you have my money, you change my ...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14639</th>
      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>
      <td>neutral</td>
    </tr>
  </tbody>
</table>
<p>14640 rows √ó 2 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>
<span class="c1">#Remove neutral class</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">airline_sentiment</span> <span class="o">!=</span> <span class="s2">&quot;neutral&quot;</span><span class="p">]</span>

<span class="c1">#text normalization</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;@[^\s]+&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">)))</span><span class="c1">#remove the name of the airline</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">((</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;[^a-zA-z0-9\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;airline_sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="s1">&#39;positive&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;airline_sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="s1">&#39;negative&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2363
9178
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>airline_sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>plus youve added commercials to the experienc...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>3</th>
      <td>its really aggressive to blast obnoxious ente...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>and its a really big bad thing about it</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>5</th>
      <td>seriously would pay 30 a flight for seats tha...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>6</th>
      <td>yes nearly every time i fly vx this ear worm ...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>14633</th>
      <td>my flight was cancelled flightled leaving tom...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14634</th>
      <td>right on cue with the delays</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14635</th>
      <td>thank you we got on a different flight to chi...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>14636</th>
      <td>leaving over 20 minutes late flight no warnin...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14638</th>
      <td>you have my money you change my flight and do...</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
<p>11541 rows √ó 2 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;rt&#39;</span><span class="p">,</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>airline_sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>plus youve added commercials to the experienc...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>3</th>
      <td>its really aggressive to blast obnoxious ente...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>4</th>
      <td>and its a really big bad thing about it</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>5</th>
      <td>seriously would pay 30 a flight for seats tha...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>6</th>
      <td>yes nearly every time i fly vx this ear worm ...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>14633</th>
      <td>my flight was cancelled flightled leaving tom...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14634</th>
      <td>right on cue with the delays</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14635</th>
      <td>thank you we got on a different flight to chi...</td>
      <td>positive</td>
    </tr>
    <tr>
      <th>14636</th>
      <td>leaving over 20 minutes late flight no warnin...</td>
      <td>negative</td>
    </tr>
    <tr>
      <th>14638</th>
      <td>you have my money you change my flight and do...</td>
      <td>negative</td>
    </tr>
  </tbody>
</table>
<p>11541 rows √ó 2 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt to /home/julian/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /home/julian/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Removing Stop Words</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="n">all_sentences</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">all_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">all_sentences</span><span class="p">]</span>
<span class="n">stop_words</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">stop_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">)):</span>  
    <span class="n">all_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-1">
<h1>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¬∂</a></h1>
<p>all_words is a list with all the tweets that are going to be used to train the model. Tokenize the tweets using a dictionary of 2000 words. Once the sentences are tokenized, take into account that the length of every tweet is different so before they can be passed to the training step, the tweets must be <strong>padded</strong> in order to provide them with equal length.</p>
<p>Review the padding function in the preprocessing module of keras and apply it to the tokenized sequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="n">max_fatures</span> <span class="o">=</span> <span class="mi">2001</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="n">NewX</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;post&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NewX</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(11541, 21)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-2">
<h1>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">¬∂</a></h1>
<p>The previous step add 0‚Äôs to some tweets in order to provide them with the same length. Now it is necessary to define a model that be able to discard those 0‚Äôs. Review the masking layer and masking option of the embedding layer of keras.</p>
<p>Define a LSTM architecture to classify the tweets as ‚Äúnegative‚Äù or ‚Äúpositive‚Äù. Use the Embedding layer and its masking option to discard the 0‚Äôs added during padding step. Evaluate the performance of the model for embed_dim = [32,64,128] and a LSTM layer with cells = [32,64,128]. Use 20% of the data for testing purposes and 10 epochs for training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;airline_sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">Encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">Encoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">GlobalMaxPooling1D</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">optimizers</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Model_Sentimen</span><span class="p">(</span><span class="n">Embeb</span><span class="p">,</span><span class="n">cells</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">Embeb</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">LSTM</span><span class="p">(</span><span class="n">cells</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">NewX</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2018</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.class_weight</span> <span class="kn">import</span> <span class="n">compute_class_weight</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="n">Ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_te</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_te</span><span class="p">[</span><span class="n">y_te</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">y_te</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">Ns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#weights = compute_class_weight(&#39;balanced&#39;, np.unique(y_tr), y_tr)</span>
<span class="c1">#weights = weights[::-1]</span>
<span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cells</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">cells</span><span class="p">)</span>
        <span class="c1">#opt = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>        
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 290us/sample - loss: 0.4047 - accuracy: 0.8315 - val_loss: 0.2892 - val_accuracy: 0.9080
Epoch 2/10
8308/8308 [==============================] - 2s 181us/sample - loss: 0.2064 - accuracy: 0.9253 - val_loss: 0.3267 - val_accuracy: 0.9058
Epoch 3/10
8308/8308 [==============================] - 1s 178us/sample - loss: 0.1544 - accuracy: 0.9405 - val_loss: 0.3455 - val_accuracy: 0.9015
Epoch 4/10
8308/8308 [==============================] - 1s 179us/sample - loss: 0.1294 - accuracy: 0.9498 - val_loss: 0.4853 - val_accuracy: 0.9026
Epoch 5/10
8308/8308 [==============================] - 2s 183us/sample - loss: 0.1093 - accuracy: 0.9573 - val_loss: 0.5668 - val_accuracy: 0.9037
Epoch 6/10
8308/8308 [==============================] - 2s 181us/sample - loss: 0.0925 - accuracy: 0.9652 - val_loss: 0.4965 - val_accuracy: 0.9026
Epoch 7/10
8308/8308 [==============================] - 1s 180us/sample - loss: 0.0848 - accuracy: 0.9682 - val_loss: 0.8476 - val_accuracy: 0.9015
Epoch 8/10
8308/8308 [==============================] - 2s 183us/sample - loss: 0.0704 - accuracy: 0.9721 - val_loss: 0.9658 - val_accuracy: 0.8994
Epoch 9/10
8308/8308 [==============================] - 1s 180us/sample - loss: 0.0652 - accuracy: 0.9765 - val_loss: 1.4710 - val_accuracy: 0.9015
Epoch 10/10
8308/8308 [==============================] - 1s 178us/sample - loss: 0.0790 - accuracy: 0.9768 - val_loss: 0.7469 - val_accuracy: 0.8994
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 308us/sample - loss: 0.3760 - accuracy: 0.8500 - val_loss: 0.2804 - val_accuracy: 0.8972
Epoch 2/10
8308/8308 [==============================] - 2s 203us/sample - loss: 0.1939 - accuracy: 0.9256 - val_loss: 0.2696 - val_accuracy: 0.9015
Epoch 3/10
8308/8308 [==============================] - 2s 205us/sample - loss: 0.1483 - accuracy: 0.9422 - val_loss: 0.3493 - val_accuracy: 0.8972
Epoch 4/10
8308/8308 [==============================] - 2s 204us/sample - loss: 0.1245 - accuracy: 0.9486 - val_loss: 0.5741 - val_accuracy: 0.9026
Epoch 5/10
8308/8308 [==============================] - 2s 203us/sample - loss: 0.1137 - accuracy: 0.9578 - val_loss: 0.4117 - val_accuracy: 0.8972
Epoch 6/10
8308/8308 [==============================] - 2s 203us/sample - loss: 0.0968 - accuracy: 0.9629 - val_loss: 0.6026 - val_accuracy: 0.9037
Epoch 7/10
8308/8308 [==============================] - 2s 204us/sample - loss: 0.0836 - accuracy: 0.9673 - val_loss: 0.6456 - val_accuracy: 0.9004
Epoch 8/10
8308/8308 [==============================] - 2s 205us/sample - loss: 0.0742 - accuracy: 0.9716 - val_loss: 1.1333 - val_accuracy: 0.9048
Epoch 9/10
8308/8308 [==============================] - 2s 202us/sample - loss: 0.0677 - accuracy: 0.9729 - val_loss: 1.5976 - val_accuracy: 0.9048
Epoch 10/10
8308/8308 [==============================] - 2s 207us/sample - loss: 0.0641 - accuracy: 0.9742 - val_loss: 1.3455 - val_accuracy: 0.9004
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 391us/sample - loss: 0.4140 - accuracy: 0.8300 - val_loss: 0.3225 - val_accuracy: 0.8983
Epoch 2/10
8308/8308 [==============================] - 2s 279us/sample - loss: 0.2060 - accuracy: 0.9271 - val_loss: 0.2701 - val_accuracy: 0.9058
Epoch 3/10
8308/8308 [==============================] - 2s 279us/sample - loss: 0.1515 - accuracy: 0.9434 - val_loss: 0.2901 - val_accuracy: 0.9048
Epoch 4/10
8308/8308 [==============================] - 2s 274us/sample - loss: 0.1271 - accuracy: 0.9550 - val_loss: 0.3733 - val_accuracy: 0.8929
Epoch 5/10
8308/8308 [==============================] - 2s 280us/sample - loss: 0.2819 - accuracy: 0.9580 - val_loss: 0.3498 - val_accuracy: 0.8994
Epoch 6/10
8308/8308 [==============================] - 2s 275us/sample - loss: 0.0890 - accuracy: 0.9655 - val_loss: 0.6065 - val_accuracy: 0.9004
Epoch 7/10
8308/8308 [==============================] - 2s 276us/sample - loss: 0.0748 - accuracy: 0.9681 - val_loss: 1.3340 - val_accuracy: 0.8918
Epoch 8/10
8308/8308 [==============================] - 2s 274us/sample - loss: 0.0669 - accuracy: 0.9717 - val_loss: 2.3839 - val_accuracy: 0.9004
Epoch 9/10
8308/8308 [==============================] - 2s 273us/sample - loss: 0.0525 - accuracy: 0.9801 - val_loss: 2.0887 - val_accuracy: 0.8896
Epoch 10/10
8308/8308 [==============================] - 2s 278us/sample - loss: 0.0466 - accuracy: 0.9800 - val_loss: 1.9959 - val_accuracy: 0.8950
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 296us/sample - loss: 0.3971 - accuracy: 0.8327 - val_loss: 0.2693 - val_accuracy: 0.9058
Epoch 2/10
8308/8308 [==============================] - 2s 186us/sample - loss: 0.2017 - accuracy: 0.9224 - val_loss: 0.2642 - val_accuracy: 0.9026
Epoch 3/10
8308/8308 [==============================] - 2s 181us/sample - loss: 0.1491 - accuracy: 0.9409 - val_loss: 0.2953 - val_accuracy: 0.9015
Epoch 4/10
8308/8308 [==============================] - 2s 182us/sample - loss: 0.1249 - accuracy: 0.9509 - val_loss: 0.3369 - val_accuracy: 0.8972
Epoch 5/10
8308/8308 [==============================] - 2s 183us/sample - loss: 0.1043 - accuracy: 0.9621 - val_loss: 0.4329 - val_accuracy: 0.9015
Epoch 6/10
8308/8308 [==============================] - 2s 185us/sample - loss: 0.0907 - accuracy: 0.9664 - val_loss: 0.4969 - val_accuracy: 0.8983
Epoch 7/10
8308/8308 [==============================] - 2s 188us/sample - loss: 0.0778 - accuracy: 0.9706 - val_loss: 0.6731 - val_accuracy: 0.8972
Epoch 8/10
8308/8308 [==============================] - 2s 184us/sample - loss: 0.0660 - accuracy: 0.9735 - val_loss: 0.9030 - val_accuracy: 0.8939
Epoch 9/10
8308/8308 [==============================] - 2s 185us/sample - loss: 0.0606 - accuracy: 0.9774 - val_loss: 0.6491 - val_accuracy: 0.8929
Epoch 10/10
8308/8308 [==============================] - 2s 182us/sample - loss: 0.0545 - accuracy: 0.9788 - val_loss: 0.5706 - val_accuracy: 0.8961
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 314us/sample - loss: 0.3691 - accuracy: 0.8420 - val_loss: 0.2849 - val_accuracy: 0.9026
Epoch 2/10
8308/8308 [==============================] - 2s 207us/sample - loss: 0.2162 - accuracy: 0.9200 - val_loss: 0.2876 - val_accuracy: 0.9058
Epoch 3/10
8308/8308 [==============================] - 2s 208us/sample - loss: 0.1640 - accuracy: 0.9446 - val_loss: 0.3342 - val_accuracy: 0.9037
Epoch 4/10
8308/8308 [==============================] - 2s 210us/sample - loss: 0.1392 - accuracy: 0.9521 - val_loss: 0.4059 - val_accuracy: 0.8983
Epoch 5/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.1241 - accuracy: 0.9599 - val_loss: 0.5818 - val_accuracy: 0.8961
Epoch 6/10
8308/8308 [==============================] - 2s 208us/sample - loss: 0.1040 - accuracy: 0.9681 - val_loss: 1.3723 - val_accuracy: 0.8907
Epoch 7/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.0956 - accuracy: 0.9677 - val_loss: 0.7685 - val_accuracy: 0.8929
Epoch 8/10
8308/8308 [==============================] - 2s 208us/sample - loss: 0.0777 - accuracy: 0.9716 - val_loss: 0.8055 - val_accuracy: 0.8842
Epoch 9/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.0660 - accuracy: 0.9754 - val_loss: 0.9694 - val_accuracy: 0.8885
Epoch 10/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.0585 - accuracy: 0.9788 - val_loss: 1.4696 - val_accuracy: 0.8810
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 385us/sample - loss: 0.3679 - accuracy: 0.8410 - val_loss: 0.2534 - val_accuracy: 0.8994
Epoch 2/10
8308/8308 [==============================] - 2s 283us/sample - loss: 0.1905 - accuracy: 0.9310 - val_loss: 0.2594 - val_accuracy: 0.9026
Epoch 3/10
8308/8308 [==============================] - 2s 283us/sample - loss: 0.1548 - accuracy: 0.9438 - val_loss: 0.2889 - val_accuracy: 0.9026
Epoch 4/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.1239 - accuracy: 0.9545 - val_loss: 0.3980 - val_accuracy: 0.9037
Epoch 5/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.1074 - accuracy: 0.9575 - val_loss: 0.4600 - val_accuracy: 0.8983
Epoch 6/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.1040 - accuracy: 0.9647 - val_loss: 0.8019 - val_accuracy: 0.8950
Epoch 7/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.0785 - accuracy: 0.9701 - val_loss: 1.5221 - val_accuracy: 0.8950
Epoch 8/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.0657 - accuracy: 0.9746 - val_loss: 1.4993 - val_accuracy: 0.8972
Epoch 9/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.0572 - accuracy: 0.9771 - val_loss: 2.1790 - val_accuracy: 0.8918
Epoch 10/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.0508 - accuracy: 0.9811 - val_loss: 2.4603 - val_accuracy: 0.8907
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 306us/sample - loss: 0.4170 - accuracy: 0.8435 - val_loss: 0.2657 - val_accuracy: 0.9069
Epoch 2/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.2110 - accuracy: 0.9249 - val_loss: 0.2666 - val_accuracy: 0.9091
Epoch 3/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.1629 - accuracy: 0.9452 - val_loss: 0.2829 - val_accuracy: 0.9026
Epoch 4/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.1321 - accuracy: 0.9562 - val_loss: 0.3714 - val_accuracy: 0.8983
Epoch 5/10
8308/8308 [==============================] - 2s 198us/sample - loss: 0.1041 - accuracy: 0.9655 - val_loss: 0.5489 - val_accuracy: 0.9015
Epoch 6/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.0944 - accuracy: 0.9687 - val_loss: 0.6692 - val_accuracy: 0.9004
Epoch 7/10
8308/8308 [==============================] - 2s 201us/sample - loss: 0.0741 - accuracy: 0.9726 - val_loss: 0.7260 - val_accuracy: 0.8929
Epoch 8/10
8308/8308 [==============================] - 2s 204us/sample - loss: 0.0642 - accuracy: 0.9768 - val_loss: 0.7598 - val_accuracy: 0.8874
Epoch 9/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.0520 - accuracy: 0.9813 - val_loss: 0.9642 - val_accuracy: 0.8983
Epoch 10/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.0427 - accuracy: 0.9831 - val_loss: 1.3138 - val_accuracy: 0.8874
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 370us/sample - loss: 0.3873 - accuracy: 0.8463 - val_loss: 0.2669 - val_accuracy: 0.9015
Epoch 2/10
8308/8308 [==============================] - 2s 230us/sample - loss: 0.1978 - accuracy: 0.9160 - val_loss: 0.2683 - val_accuracy: 0.9015
Epoch 3/10
8308/8308 [==============================] - 2s 224us/sample - loss: 0.1554 - accuracy: 0.9381 - val_loss: 0.3247 - val_accuracy: 0.8983
Epoch 4/10
8308/8308 [==============================] - 2s 227us/sample - loss: 0.1275 - accuracy: 0.9515 - val_loss: 0.3928 - val_accuracy: 0.9015
Epoch 5/10
8308/8308 [==============================] - 2s 227us/sample - loss: 0.1134 - accuracy: 0.9585 - val_loss: 0.3126 - val_accuracy: 0.8994
Epoch 6/10
8308/8308 [==============================] - 2s 226us/sample - loss: 0.0979 - accuracy: 0.9643 - val_loss: 0.4849 - val_accuracy: 0.8907
Epoch 7/10
8308/8308 [==============================] - 2s 225us/sample - loss: 0.0825 - accuracy: 0.9688 - val_loss: 0.4976 - val_accuracy: 0.8788
Epoch 8/10
8308/8308 [==============================] - 2s 232us/sample - loss: 0.0683 - accuracy: 0.9748 - val_loss: 0.9601 - val_accuracy: 0.8831
Epoch 9/10
8308/8308 [==============================] - 2s 234us/sample - loss: 0.0666 - accuracy: 0.9742 - val_loss: 0.7940 - val_accuracy: 0.8896
Epoch 10/10
8308/8308 [==============================] - 2s 227us/sample - loss: 0.0580 - accuracy: 0.9788 - val_loss: 0.8704 - val_accuracy: 0.8939
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 410us/sample - loss: 0.4433 - accuracy: 0.8392 - val_loss: 0.2771 - val_accuracy: 0.9069
Epoch 2/10
8308/8308 [==============================] - 3s 308us/sample - loss: 0.2176 - accuracy: 0.9233 - val_loss: 0.3349 - val_accuracy: 0.8983
Epoch 3/10
8308/8308 [==============================] - 3s 309us/sample - loss: 0.1632 - accuracy: 0.9411 - val_loss: 0.2863 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 3s 307us/sample - loss: 0.1274 - accuracy: 0.9537 - val_loss: 0.5101 - val_accuracy: 0.9004
Epoch 5/10
8308/8308 [==============================] - 3s 306us/sample - loss: 0.1079 - accuracy: 0.9594 - val_loss: 1.1722 - val_accuracy: 0.8939
Epoch 6/10
8308/8308 [==============================] - 3s 306us/sample - loss: 0.0908 - accuracy: 0.9675 - val_loss: 1.2106 - val_accuracy: 0.8950
Epoch 7/10
8308/8308 [==============================] - 3s 306us/sample - loss: 0.0750 - accuracy: 0.9733 - val_loss: 0.9782 - val_accuracy: 0.8950
Epoch 8/10
8308/8308 [==============================] - 3s 307us/sample - loss: 0.0789 - accuracy: 0.9762 - val_loss: 2.4894 - val_accuracy: 0.8896
Epoch 9/10
8308/8308 [==============================] - 3s 306us/sample - loss: 0.0720 - accuracy: 0.9750 - val_loss: 1.4375 - val_accuracy: 0.8961
Epoch 10/10
8308/8308 [==============================] - 3s 307us/sample - loss: 0.0509 - accuracy: 0.9805 - val_loss: 1.8857 - val_accuracy: 0.9004
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy= </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_26_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_26_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy= 0.8973581637072325
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-3">
<h1>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">¬∂</a></h1>
<p>Train a CBOW model with the dataset using and Embedding dimension of [32,64,128]. Create a new network and transfer the pretrained weights to the Embedding layer. Use the same architecture than before (except for the Embedding layer). Compare the results.</p>
<p><strong>Note</strong>: Take care on the tokenization of the words. Keras tokenizer does not assign the zero value to any word because of padding purposes. Make sure that the order of the vectors in the GloVe embedding matrix corresponds with the indexs used to represent the words in the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.sklearn_api</span> <span class="kn">import</span> <span class="n">W2VTransformer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Model_Sentimen</span><span class="p">(</span><span class="n">Embeb</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">Embeb</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">Embeb</span><span class="p">)),</span><span class="n">wordsmatrix</span><span class="p">]],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">LSTM</span><span class="p">(</span><span class="n">cells</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cells</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
        <span class="c1">#---------------------------------------------------------------------------------</span>
        <span class="n">model_CBOW</span> <span class="o">=</span> <span class="n">W2VTransformer</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">window</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">max_vocab_size</span><span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span><span class="p">),</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sg</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">iter</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
        <span class="n">model_CBOW</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">all_words</span><span class="p">)</span>
        <span class="n">wordsmatrix</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">2000</span><span class="p">])</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">)</span>
        <span class="c1">#opt = optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>     
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 287us/sample - loss: 0.4109 - accuracy: 0.8303 - val_loss: 0.3080 - val_accuracy: 0.8777
Epoch 2/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2982 - accuracy: 0.8914 - val_loss: 0.3073 - val_accuracy: 0.8766
Epoch 3/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2652 - accuracy: 0.8997 - val_loss: 0.2912 - val_accuracy: 0.8820
Epoch 4/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2475 - accuracy: 0.9096 - val_loss: 0.2983 - val_accuracy: 0.8810
Epoch 5/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2355 - accuracy: 0.9108 - val_loss: 0.2859 - val_accuracy: 0.8853
Epoch 6/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2168 - accuracy: 0.9160 - val_loss: 0.2953 - val_accuracy: 0.8810
Epoch 7/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.2047 - accuracy: 0.9254 - val_loss: 0.3631 - val_accuracy: 0.8788
Epoch 8/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.1891 - accuracy: 0.9269 - val_loss: 0.3566 - val_accuracy: 0.8777
Epoch 9/10
8308/8308 [==============================] - 1s 166us/sample - loss: 0.1763 - accuracy: 0.9331 - val_loss: 0.3399 - val_accuracy: 0.8853
Epoch 10/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1622 - accuracy: 0.9390 - val_loss: 0.3462 - val_accuracy: 0.8799
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 293us/sample - loss: 0.3531 - accuracy: 0.8576 - val_loss: 0.3054 - val_accuracy: 0.8885
Epoch 2/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2626 - accuracy: 0.8994 - val_loss: 0.3047 - val_accuracy: 0.8755
Epoch 3/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.2350 - accuracy: 0.9101 - val_loss: 0.2813 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 2s 187us/sample - loss: 0.2112 - accuracy: 0.9182 - val_loss: 0.2940 - val_accuracy: 0.8864
Epoch 5/10
8308/8308 [==============================] - 2s 188us/sample - loss: 0.1896 - accuracy: 0.9261 - val_loss: 0.3065 - val_accuracy: 0.8853
Epoch 6/10
8308/8308 [==============================] - 2s 188us/sample - loss: 0.1704 - accuracy: 0.9312 - val_loss: 0.3501 - val_accuracy: 0.8853
Epoch 7/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.1494 - accuracy: 0.9401 - val_loss: 0.3170 - val_accuracy: 0.8864
Epoch 8/10
8308/8308 [==============================] - 2s 187us/sample - loss: 0.1325 - accuracy: 0.9455 - val_loss: 0.4079 - val_accuracy: 0.8853
Epoch 9/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.1124 - accuracy: 0.9541 - val_loss: 0.4130 - val_accuracy: 0.8874
Epoch 10/10
8308/8308 [==============================] - 2s 188us/sample - loss: 0.0998 - accuracy: 0.9579 - val_loss: 0.4642 - val_accuracy: 0.8896
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 359us/sample - loss: 0.3623 - accuracy: 0.8619 - val_loss: 0.2960 - val_accuracy: 0.8853
Epoch 2/10
8308/8308 [==============================] - 2s 261us/sample - loss: 0.2845 - accuracy: 0.8991 - val_loss: 0.2948 - val_accuracy: 0.8777
Epoch 3/10
8308/8308 [==============================] - 2s 265us/sample - loss: 0.2530 - accuracy: 0.9130 - val_loss: 0.2972 - val_accuracy: 0.8885
Epoch 4/10
8308/8308 [==============================] - 2s 263us/sample - loss: 0.2276 - accuracy: 0.9214 - val_loss: 0.2660 - val_accuracy: 0.8950
Epoch 5/10
8308/8308 [==============================] - 2s 264us/sample - loss: 0.2075 - accuracy: 0.9313 - val_loss: 0.2975 - val_accuracy: 0.8950
Epoch 6/10
8308/8308 [==============================] - 2s 261us/sample - loss: 0.1771 - accuracy: 0.9391 - val_loss: 0.4138 - val_accuracy: 0.8896
Epoch 7/10
8308/8308 [==============================] - 2s 261us/sample - loss: 0.1540 - accuracy: 0.9457 - val_loss: 0.4187 - val_accuracy: 0.8907
Epoch 8/10
8308/8308 [==============================] - 2s 267us/sample - loss: 0.1292 - accuracy: 0.9574 - val_loss: 0.4138 - val_accuracy: 0.8842
Epoch 9/10
8308/8308 [==============================] - 2s 261us/sample - loss: 0.1173 - accuracy: 0.9627 - val_loss: 0.4233 - val_accuracy: 0.8961
Epoch 10/10
8308/8308 [==============================] - 2s 261us/sample - loss: 0.0961 - accuracy: 0.9727 - val_loss: 0.4772 - val_accuracy: 0.8939
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.3783 - accuracy: 0.8427 - val_loss: 0.3164 - val_accuracy: 0.8777
Epoch 2/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.2678 - accuracy: 0.8988 - val_loss: 0.2839 - val_accuracy: 0.8929
Epoch 3/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.2321 - accuracy: 0.9102 - val_loss: 0.3107 - val_accuracy: 0.8961
Epoch 4/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.2032 - accuracy: 0.9226 - val_loss: 0.2916 - val_accuracy: 0.8853
Epoch 5/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1813 - accuracy: 0.9303 - val_loss: 0.3201 - val_accuracy: 0.8885
Epoch 6/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1655 - accuracy: 0.9377 - val_loss: 0.3239 - val_accuracy: 0.8842
Epoch 7/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1443 - accuracy: 0.9456 - val_loss: 0.4279 - val_accuracy: 0.8755
Epoch 8/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1249 - accuracy: 0.9527 - val_loss: 0.4285 - val_accuracy: 0.8680
Epoch 9/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1090 - accuracy: 0.9599 - val_loss: 0.4891 - val_accuracy: 0.8799
Epoch 10/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.0911 - accuracy: 0.9646 - val_loss: 0.5885 - val_accuracy: 0.8810
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 304us/sample - loss: 0.3478 - accuracy: 0.8546 - val_loss: 0.2855 - val_accuracy: 0.8885
Epoch 2/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.2523 - accuracy: 0.8962 - val_loss: 0.2747 - val_accuracy: 0.8820
Epoch 3/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.2196 - accuracy: 0.9178 - val_loss: 0.2916 - val_accuracy: 0.8918
Epoch 4/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.1889 - accuracy: 0.9262 - val_loss: 0.2821 - val_accuracy: 0.8799
Epoch 5/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.1626 - accuracy: 0.9386 - val_loss: 0.2857 - val_accuracy: 0.8831
Epoch 6/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.1351 - accuracy: 0.9509 - val_loss: 0.3488 - val_accuracy: 0.8907
Epoch 7/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.1040 - accuracy: 0.9621 - val_loss: 0.4513 - val_accuracy: 0.8907
Epoch 8/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.0983 - accuracy: 0.9629 - val_loss: 0.4047 - val_accuracy: 0.8896
Epoch 9/10
8308/8308 [==============================] - 2s 189us/sample - loss: 0.0755 - accuracy: 0.9709 - val_loss: 0.4581 - val_accuracy: 0.8907
Epoch 10/10
8308/8308 [==============================] - 2s 190us/sample - loss: 0.0525 - accuracy: 0.9810 - val_loss: 0.5487 - val_accuracy: 0.8874
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 364us/sample - loss: 0.3441 - accuracy: 0.8666 - val_loss: 0.2733 - val_accuracy: 0.8907
Epoch 2/10
8308/8308 [==============================] - 2s 264us/sample - loss: 0.2455 - accuracy: 0.9068 - val_loss: 0.2564 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 2s 263us/sample - loss: 0.2076 - accuracy: 0.9227 - val_loss: 0.2886 - val_accuracy: 0.8972
Epoch 4/10
8308/8308 [==============================] - 2s 262us/sample - loss: 0.1713 - accuracy: 0.9349 - val_loss: 0.2918 - val_accuracy: 0.8874
Epoch 5/10
8308/8308 [==============================] - 2s 263us/sample - loss: 0.1403 - accuracy: 0.9486 - val_loss: 0.3964 - val_accuracy: 0.8853
Epoch 6/10
8308/8308 [==============================] - 2s 263us/sample - loss: 0.1212 - accuracy: 0.9570 - val_loss: 0.3979 - val_accuracy: 0.8885
Epoch 7/10
8308/8308 [==============================] - 2s 269us/sample - loss: 0.0963 - accuracy: 0.9661 - val_loss: 0.5620 - val_accuracy: 0.8831
Epoch 8/10
8308/8308 [==============================] - 2s 268us/sample - loss: 0.0738 - accuracy: 0.9732 - val_loss: 0.4682 - val_accuracy: 0.8810
Epoch 9/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.0608 - accuracy: 0.9799 - val_loss: 0.4919 - val_accuracy: 0.8842
Epoch 10/10
8308/8308 [==============================] - 2s 269us/sample - loss: 0.0491 - accuracy: 0.9845 - val_loss: 0.5858 - val_accuracy: 0.8788
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 274us/sample - loss: 0.3745 - accuracy: 0.8458 - val_loss: 0.2915 - val_accuracy: 0.8766
Epoch 2/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2511 - accuracy: 0.9032 - val_loss: 0.2867 - val_accuracy: 0.8864
Epoch 3/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2125 - accuracy: 0.9163 - val_loss: 0.2956 - val_accuracy: 0.8874
Epoch 4/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1780 - accuracy: 0.9330 - val_loss: 0.3145 - val_accuracy: 0.8907
Epoch 5/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.1543 - accuracy: 0.9387 - val_loss: 0.4043 - val_accuracy: 0.8918
Epoch 6/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1221 - accuracy: 0.9535 - val_loss: 0.4016 - val_accuracy: 0.8885
Epoch 7/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.1009 - accuracy: 0.9609 - val_loss: 0.4250 - val_accuracy: 0.8799
Epoch 8/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.0802 - accuracy: 0.9682 - val_loss: 0.6039 - val_accuracy: 0.8788
Epoch 9/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.0775 - accuracy: 0.9709 - val_loss: 0.6397 - val_accuracy: 0.8853
Epoch 10/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.0541 - accuracy: 0.9797 - val_loss: 0.6568 - val_accuracy: 0.8831
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 297us/sample - loss: 0.3429 - accuracy: 0.8652 - val_loss: 0.2820 - val_accuracy: 0.8972
Epoch 2/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2266 - accuracy: 0.9136 - val_loss: 0.2941 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1915 - accuracy: 0.9251 - val_loss: 0.2997 - val_accuracy: 0.8820
Epoch 4/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1560 - accuracy: 0.9385 - val_loss: 0.3335 - val_accuracy: 0.8799
Epoch 5/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1233 - accuracy: 0.9508 - val_loss: 0.3494 - val_accuracy: 0.8788
Epoch 6/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1023 - accuracy: 0.9598 - val_loss: 0.3974 - val_accuracy: 0.8842
Epoch 7/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.0713 - accuracy: 0.9717 - val_loss: 0.5352 - val_accuracy: 0.8896
Epoch 8/10
8308/8308 [==============================] - 2s 200us/sample - loss: 0.0594 - accuracy: 0.9775 - val_loss: 0.5440 - val_accuracy: 0.8842
Epoch 9/10
8308/8308 [==============================] - 2s 200us/sample - loss: 0.0513 - accuracy: 0.9813 - val_loss: 0.5504 - val_accuracy: 0.8831
Epoch 10/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.0363 - accuracy: 0.9860 - val_loss: 0.8221 - val_accuracy: 0.8777
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 390us/sample - loss: 0.3535 - accuracy: 0.8748 - val_loss: 0.2745 - val_accuracy: 0.8831
Epoch 2/10
8308/8308 [==============================] - 2s 288us/sample - loss: 0.2644 - accuracy: 0.9124 - val_loss: 0.2636 - val_accuracy: 0.8907
Epoch 3/10
8308/8308 [==============================] - 2s 297us/sample - loss: 0.2208 - accuracy: 0.9260 - val_loss: 0.2669 - val_accuracy: 0.8929
Epoch 4/10
8308/8308 [==============================] - 2s 298us/sample - loss: 0.1829 - accuracy: 0.9435 - val_loss: 0.2969 - val_accuracy: 0.8907
Epoch 5/10
8308/8308 [==============================] - 2s 297us/sample - loss: 0.1443 - accuracy: 0.9561 - val_loss: 0.3410 - val_accuracy: 0.8864
Epoch 6/10
8308/8308 [==============================] - 2s 294us/sample - loss: 0.1132 - accuracy: 0.9693 - val_loss: 0.3916 - val_accuracy: 0.8853
Epoch 7/10
8308/8308 [==============================] - 2s 299us/sample - loss: 0.0866 - accuracy: 0.9769 - val_loss: 0.4372 - val_accuracy: 0.8907
Epoch 8/10
8308/8308 [==============================] - 2s 291us/sample - loss: 0.0687 - accuracy: 0.9838 - val_loss: 0.5106 - val_accuracy: 0.8853
Epoch 9/10
8308/8308 [==============================] - 2s 293us/sample - loss: 0.0575 - accuracy: 0.9886 - val_loss: 0.5068 - val_accuracy: 0.8907
Epoch 10/10
8308/8308 [==============================] - 2s 288us/sample - loss: 0.0576 - accuracy: 0.9870 - val_loss: 0.5198 - val_accuracy: 0.8885
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy= </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_32_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_32_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy= 0.9034213945430922
</pre></div>
</div>
</div>
</div>
<div class="section" id="prueba-entrenando-glove">
<h2>Prueba entrenando GloVe<a class="headerlink" href="#prueba-entrenando-glove" title="Permalink to this headline">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_sentences2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">)):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">all_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">sen</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)):</span>
        <span class="n">sen</span> <span class="o">=</span> <span class="n">sen</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span><span class="n">temp</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">all_sentences2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sen</span><span class="p">)</span>
        
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">local.lib.glove</span> <span class="kn">import</span> <span class="n">evaluate</span>
<span class="kn">from</span> <span class="nn">local.lib.glove</span> <span class="kn">import</span> <span class="n">glove</span>

<span class="n">glove</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">all_sentences2</span><span class="p">)</span>    
<span class="c1">#Coocurrence matrix</span>
<span class="n">cooccur</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">build_cooccur</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">all_sentences2</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">id2word</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">make_id2word</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cells</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
        <span class="c1">#---------------------------------------------------------------------------------</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">train_glove</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">cooccur</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
        <span class="c1"># Merge and normalize word vectors</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">merge_main_context</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">indice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
            <span class="n">indice</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">k</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">wordsmatrix</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">indice</span><span class="p">,:]</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>     
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 3s 404us/sample - loss: 0.5264 - accuracy: 0.7916 - val_loss: 0.4953 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 1s 176us/sample - loss: 0.4920 - accuracy: 0.7938 - val_loss: 0.4924 - val_accuracy: 0.7963
Epoch 3/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4805 - accuracy: 0.7947 - val_loss: 0.4847 - val_accuracy: 0.7835
Epoch 4/30
7724/7724 [==============================] - 1s 172us/sample - loss: 0.4734 - accuracy: 0.7956 - val_loss: 0.4971 - val_accuracy: 0.7905
Epoch 5/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4683 - accuracy: 0.7958 - val_loss: 0.4684 - val_accuracy: 0.7963
Epoch 6/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4588 - accuracy: 0.7982 - val_loss: 0.4619 - val_accuracy: 0.8079
Epoch 7/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4526 - accuracy: 0.8071 - val_loss: 0.4546 - val_accuracy: 0.8091
Epoch 8/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4519 - accuracy: 0.8071 - val_loss: 0.4595 - val_accuracy: 0.8033
Epoch 9/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4458 - accuracy: 0.8116 - val_loss: 0.4444 - val_accuracy: 0.8219
Epoch 10/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4430 - accuracy: 0.8115 - val_loss: 0.4474 - val_accuracy: 0.8219
Epoch 11/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4374 - accuracy: 0.8150 - val_loss: 0.4403 - val_accuracy: 0.8161
Epoch 12/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4363 - accuracy: 0.8132 - val_loss: 0.4328 - val_accuracy: 0.8219
Epoch 13/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4323 - accuracy: 0.8146 - val_loss: 0.4316 - val_accuracy: 0.8231
Epoch 14/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4304 - accuracy: 0.8146 - val_loss: 0.4407 - val_accuracy: 0.8254
Epoch 15/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4272 - accuracy: 0.8160 - val_loss: 0.4276 - val_accuracy: 0.8242
Epoch 16/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4209 - accuracy: 0.8190 - val_loss: 0.4173 - val_accuracy: 0.8265
Epoch 17/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.4201 - accuracy: 0.8193 - val_loss: 0.4173 - val_accuracy: 0.8265
Epoch 18/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.4148 - accuracy: 0.8197 - val_loss: 0.4302 - val_accuracy: 0.8207
Epoch 19/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.4135 - accuracy: 0.8213 - val_loss: 0.4322 - val_accuracy: 0.8254
Epoch 20/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.4121 - accuracy: 0.8206 - val_loss: 0.4154 - val_accuracy: 0.8312
Epoch 21/30
7724/7724 [==============================] - 1s 166us/sample - loss: 0.4086 - accuracy: 0.8230 - val_loss: 0.4166 - val_accuracy: 0.8289
Epoch 22/30
7724/7724 [==============================] - 1s 165us/sample - loss: 0.4063 - accuracy: 0.8224 - val_loss: 0.4185 - val_accuracy: 0.8324
Epoch 23/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.4041 - accuracy: 0.8243 - val_loss: 0.4231 - val_accuracy: 0.8335
Epoch 24/30
7724/7724 [==============================] - 1s 166us/sample - loss: 0.4052 - accuracy: 0.8215 - val_loss: 0.4115 - val_accuracy: 0.8324
Epoch 25/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.4002 - accuracy: 0.8229 - val_loss: 0.4201 - val_accuracy: 0.8335
Epoch 26/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3988 - accuracy: 0.8235 - val_loss: 0.4088 - val_accuracy: 0.8324
Epoch 27/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3956 - accuracy: 0.8247 - val_loss: 0.4701 - val_accuracy: 0.8254
Epoch 28/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3927 - accuracy: 0.8268 - val_loss: 0.4020 - val_accuracy: 0.8324
Epoch 29/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.3922 - accuracy: 0.8272 - val_loss: 0.4358 - val_accuracy: 0.8312
Epoch 30/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.3914 - accuracy: 0.8263 - val_loss: 0.4181 - val_accuracy: 0.8289
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 2s 303us/sample - loss: 0.5192 - accuracy: 0.7929 - val_loss: 0.5045 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4849 - accuracy: 0.7931 - val_loss: 0.5071 - val_accuracy: 0.7905
Epoch 3/30
7724/7724 [==============================] - 2s 202us/sample - loss: 0.4740 - accuracy: 0.7931 - val_loss: 0.5121 - val_accuracy: 0.7905
Epoch 4/30
7724/7724 [==============================] - 2s 203us/sample - loss: 0.4743 - accuracy: 0.7935 - val_loss: 0.5215 - val_accuracy: 0.7905
Epoch 5/30
7724/7724 [==============================] - 2s 202us/sample - loss: 0.4677 - accuracy: 0.7953 - val_loss: 0.4816 - val_accuracy: 0.7986
Epoch 6/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4572 - accuracy: 0.7983 - val_loss: 0.4939 - val_accuracy: 0.7963
Epoch 7/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.4493 - accuracy: 0.8053 - val_loss: 0.4500 - val_accuracy: 0.8172
Epoch 8/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.4416 - accuracy: 0.8105 - val_loss: 0.4581 - val_accuracy: 0.8184
Epoch 9/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4319 - accuracy: 0.8129 - val_loss: 0.4462 - val_accuracy: 0.8219
Epoch 10/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4290 - accuracy: 0.8176 - val_loss: 0.4294 - val_accuracy: 0.8219
Epoch 11/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4249 - accuracy: 0.8153 - val_loss: 0.4291 - val_accuracy: 0.8242
Epoch 12/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4151 - accuracy: 0.8198 - val_loss: 0.4364 - val_accuracy: 0.8137
Epoch 13/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.4123 - accuracy: 0.8230 - val_loss: 0.4430 - val_accuracy: 0.8265
Epoch 14/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.4058 - accuracy: 0.8244 - val_loss: 0.4440 - val_accuracy: 0.8254
Epoch 15/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.4048 - accuracy: 0.8225 - val_loss: 0.4238 - val_accuracy: 0.8277
Epoch 16/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.3996 - accuracy: 0.8244 - val_loss: 0.4330 - val_accuracy: 0.8289
Epoch 17/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.3952 - accuracy: 0.8251 - val_loss: 0.4499 - val_accuracy: 0.8254
Epoch 18/30
7724/7724 [==============================] - 2s 201us/sample - loss: 0.3917 - accuracy: 0.8287 - val_loss: 0.4547 - val_accuracy: 0.8289
Epoch 19/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.3902 - accuracy: 0.8269 - val_loss: 0.4449 - val_accuracy: 0.8207
Epoch 20/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.3851 - accuracy: 0.8282 - val_loss: 0.4392 - val_accuracy: 0.8359
Epoch 21/30
7724/7724 [==============================] - 2s 198us/sample - loss: 0.3833 - accuracy: 0.8288 - val_loss: 0.4629 - val_accuracy: 0.8068
Epoch 22/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.3833 - accuracy: 0.8288 - val_loss: 0.4430 - val_accuracy: 0.8300
Epoch 23/30
7724/7724 [==============================] - 2s 200us/sample - loss: 0.3758 - accuracy: 0.8322 - val_loss: 0.4446 - val_accuracy: 0.8335
Epoch 24/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.3741 - accuracy: 0.8294 - val_loss: 0.4457 - val_accuracy: 0.8079
Epoch 25/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.3731 - accuracy: 0.8334 - val_loss: 0.5074 - val_accuracy: 0.8312
Epoch 26/30
7724/7724 [==============================] - 2s 198us/sample - loss: 0.3777 - accuracy: 0.8277 - val_loss: 0.4393 - val_accuracy: 0.8289
Epoch 27/30
7724/7724 [==============================] - 2s 199us/sample - loss: 0.3662 - accuracy: 0.8327 - val_loss: 0.4495 - val_accuracy: 0.8056
Epoch 28/30
7724/7724 [==============================] - 2s 198us/sample - loss: 0.3619 - accuracy: 0.8348 - val_loss: 0.4560 - val_accuracy: 0.8161
Epoch 29/30
7724/7724 [==============================] - 2s 198us/sample - loss: 0.3600 - accuracy: 0.8376 - val_loss: 0.4437 - val_accuracy: 0.8161
Epoch 30/30
7724/7724 [==============================] - 2s 197us/sample - loss: 0.3546 - accuracy: 0.8364 - val_loss: 0.4733 - val_accuracy: 0.8231
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 3s 375us/sample - loss: 0.5149 - accuracy: 0.7903 - val_loss: 0.4928 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 2s 273us/sample - loss: 0.4892 - accuracy: 0.7931 - val_loss: 0.5087 - val_accuracy: 0.7905
Epoch 3/30
7724/7724 [==============================] - 2s 274us/sample - loss: 0.4777 - accuracy: 0.7944 - val_loss: 0.4743 - val_accuracy: 0.7963
Epoch 4/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4749 - accuracy: 0.7949 - val_loss: 0.4809 - val_accuracy: 0.7905
Epoch 5/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4631 - accuracy: 0.7947 - val_loss: 0.4667 - val_accuracy: 0.7963
Epoch 6/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4568 - accuracy: 0.7976 - val_loss: 0.4717 - val_accuracy: 0.7963
Epoch 7/30
7724/7724 [==============================] - 2s 273us/sample - loss: 0.4497 - accuracy: 0.7996 - val_loss: 0.4704 - val_accuracy: 0.8033
Epoch 8/30
7724/7724 [==============================] - 2s 273us/sample - loss: 0.4445 - accuracy: 0.8039 - val_loss: 0.4441 - val_accuracy: 0.8149
Epoch 9/30
7724/7724 [==============================] - 2s 271us/sample - loss: 0.4346 - accuracy: 0.8133 - val_loss: 0.4370 - val_accuracy: 0.8161
Epoch 10/30
7724/7724 [==============================] - 2s 271us/sample - loss: 0.4284 - accuracy: 0.8164 - val_loss: 0.4276 - val_accuracy: 0.8254
Epoch 11/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4208 - accuracy: 0.8165 - val_loss: 0.4256 - val_accuracy: 0.8277
Epoch 12/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4198 - accuracy: 0.8186 - val_loss: 0.4416 - val_accuracy: 0.8196
Epoch 13/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4100 - accuracy: 0.8209 - val_loss: 0.4263 - val_accuracy: 0.8231
Epoch 14/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4179 - accuracy: 0.8165 - val_loss: 0.4216 - val_accuracy: 0.8265
Epoch 15/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4086 - accuracy: 0.8242 - val_loss: 0.4273 - val_accuracy: 0.8300
Epoch 16/30
7724/7724 [==============================] - 2s 273us/sample - loss: 0.4020 - accuracy: 0.8229 - val_loss: 0.4273 - val_accuracy: 0.8231
Epoch 17/30
7724/7724 [==============================] - 2s 274us/sample - loss: 0.3969 - accuracy: 0.8242 - val_loss: 0.4101 - val_accuracy: 0.8231
Epoch 18/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3927 - accuracy: 0.8256 - val_loss: 0.4132 - val_accuracy: 0.8300
Epoch 19/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.4300 - accuracy: 0.8230 - val_loss: 0.4274 - val_accuracy: 0.8254
Epoch 20/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3924 - accuracy: 0.8281 - val_loss: 0.4281 - val_accuracy: 0.8300
Epoch 21/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3913 - accuracy: 0.8298 - val_loss: 0.4328 - val_accuracy: 0.8254
Epoch 22/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3859 - accuracy: 0.8290 - val_loss: 0.4219 - val_accuracy: 0.8324
Epoch 23/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3782 - accuracy: 0.8305 - val_loss: 0.4166 - val_accuracy: 0.8265
Epoch 24/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3775 - accuracy: 0.8308 - val_loss: 0.4582 - val_accuracy: 0.8359
Epoch 25/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3703 - accuracy: 0.8348 - val_loss: 0.4471 - val_accuracy: 0.8161
Epoch 26/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3698 - accuracy: 0.8322 - val_loss: 0.4308 - val_accuracy: 0.8265
Epoch 27/30
7724/7724 [==============================] - 2s 273us/sample - loss: 0.3689 - accuracy: 0.8352 - val_loss: 0.4475 - val_accuracy: 0.8184
Epoch 28/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3615 - accuracy: 0.8375 - val_loss: 0.4262 - val_accuracy: 0.8207
Epoch 29/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3635 - accuracy: 0.8379 - val_loss: 0.4559 - val_accuracy: 0.8265
Epoch 30/30
7724/7724 [==============================] - 2s 272us/sample - loss: 0.3605 - accuracy: 0.8380 - val_loss: 0.4808 - val_accuracy: 0.8219
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 2s 279us/sample - loss: 0.5303 - accuracy: 0.7829 - val_loss: 0.5022 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 1s 172us/sample - loss: 0.4893 - accuracy: 0.7935 - val_loss: 0.4933 - val_accuracy: 0.7905
Epoch 3/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4767 - accuracy: 0.7965 - val_loss: 0.6199 - val_accuracy: 0.7905
Epoch 4/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4706 - accuracy: 0.7961 - val_loss: 0.7062 - val_accuracy: 0.7963
Epoch 5/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4672 - accuracy: 0.7970 - val_loss: 0.5718 - val_accuracy: 0.7986
Epoch 6/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4594 - accuracy: 0.7973 - val_loss: 0.5472 - val_accuracy: 0.8009
Epoch 7/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4505 - accuracy: 0.7980 - val_loss: 0.5106 - val_accuracy: 0.8009
Epoch 8/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4446 - accuracy: 0.8000 - val_loss: 0.4619 - val_accuracy: 0.8009
Epoch 9/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4449 - accuracy: 0.8005 - val_loss: 0.4501 - val_accuracy: 0.8149
Epoch 10/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4335 - accuracy: 0.8086 - val_loss: 0.4675 - val_accuracy: 0.8184
Epoch 11/30
7724/7724 [==============================] - 1s 172us/sample - loss: 0.4301 - accuracy: 0.8118 - val_loss: 0.4387 - val_accuracy: 0.8242
Epoch 12/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.4291 - accuracy: 0.8108 - val_loss: 0.4417 - val_accuracy: 0.8207
Epoch 13/30
7724/7724 [==============================] - 1s 169us/sample - loss: 0.4217 - accuracy: 0.8133 - val_loss: 0.4453 - val_accuracy: 0.8196
Epoch 14/30
7724/7724 [==============================] - 1s 169us/sample - loss: 0.4163 - accuracy: 0.8187 - val_loss: 0.4370 - val_accuracy: 0.8231
Epoch 15/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.4115 - accuracy: 0.8260 - val_loss: 0.4418 - val_accuracy: 0.8324
Epoch 16/30
7724/7724 [==============================] - 1s 171us/sample - loss: 0.4114 - accuracy: 0.8244 - val_loss: 0.4364 - val_accuracy: 0.8137
Epoch 17/30
7724/7724 [==============================] - 1s 170us/sample - loss: 0.4076 - accuracy: 0.8226 - val_loss: 0.4503 - val_accuracy: 0.8172
Epoch 18/30
7724/7724 [==============================] - 1s 169us/sample - loss: 0.4040 - accuracy: 0.8266 - val_loss: 0.4305 - val_accuracy: 0.8254
Epoch 19/30
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7724/7724 [==============================] - 1s 168us/sample - loss: 0.4005 - accuracy: 0.8246 - val_loss: 0.4417 - val_accuracy: 0.8149
Epoch 20/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3979 - accuracy: 0.8272 - val_loss: 0.4470 - val_accuracy: 0.8207
Epoch 21/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3952 - accuracy: 0.8291 - val_loss: 0.4390 - val_accuracy: 0.8265
Epoch 22/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3922 - accuracy: 0.8285 - val_loss: 0.4421 - val_accuracy: 0.8056
Epoch 23/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3906 - accuracy: 0.8314 - val_loss: 0.4376 - val_accuracy: 0.8184
Epoch 24/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3859 - accuracy: 0.8334 - val_loss: 0.4635 - val_accuracy: 0.8102
Epoch 25/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3851 - accuracy: 0.8336 - val_loss: 0.4253 - val_accuracy: 0.8172
Epoch 26/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3853 - accuracy: 0.8323 - val_loss: 0.4276 - val_accuracy: 0.8207
Epoch 27/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3804 - accuracy: 0.8344 - val_loss: 0.4508 - val_accuracy: 0.8312
Epoch 28/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3799 - accuracy: 0.8351 - val_loss: 0.4339 - val_accuracy: 0.8149
Epoch 29/30
7724/7724 [==============================] - 1s 167us/sample - loss: 0.3786 - accuracy: 0.8352 - val_loss: 0.4410 - val_accuracy: 0.8265
Epoch 30/30
7724/7724 [==============================] - 1s 168us/sample - loss: 0.3741 - accuracy: 0.8378 - val_loss: 0.4464 - val_accuracy: 0.8289
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 3s 336us/sample - loss: 0.5098 - accuracy: 0.7931 - val_loss: 0.4922 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 2s 203us/sample - loss: 0.4878 - accuracy: 0.7954 - val_loss: 0.4830 - val_accuracy: 0.7963
Epoch 3/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4895 - accuracy: 0.7965 - val_loss: 0.4925 - val_accuracy: 0.7963
Epoch 4/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4817 - accuracy: 0.7963 - val_loss: 0.4799 - val_accuracy: 0.7963
Epoch 5/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4866 - accuracy: 0.7953 - val_loss: 0.4752 - val_accuracy: 0.7986
Epoch 6/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.4658 - accuracy: 0.7973 - val_loss: 0.4722 - val_accuracy: 0.7986
Epoch 7/30
7724/7724 [==============================] - 2s 203us/sample - loss: 0.4550 - accuracy: 0.8010 - val_loss: 0.4550 - val_accuracy: 0.8068
Epoch 8/30
7724/7724 [==============================] - 2s 203us/sample - loss: 0.4485 - accuracy: 0.8032 - val_loss: 0.4550 - val_accuracy: 0.8149
Epoch 9/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4404 - accuracy: 0.8074 - val_loss: 0.4489 - val_accuracy: 0.8161
Epoch 10/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4284 - accuracy: 0.8112 - val_loss: 0.4491 - val_accuracy: 0.8161
Epoch 11/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4230 - accuracy: 0.8114 - val_loss: 0.4359 - val_accuracy: 0.8184
Epoch 12/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4229 - accuracy: 0.8162 - val_loss: 0.4365 - val_accuracy: 0.8137
Epoch 13/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4151 - accuracy: 0.8160 - val_loss: 0.4300 - val_accuracy: 0.8265
Epoch 14/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.4084 - accuracy: 0.8208 - val_loss: 0.4350 - val_accuracy: 0.8161
Epoch 15/30
7724/7724 [==============================] - 2s 203us/sample - loss: 0.4076 - accuracy: 0.8199 - val_loss: 0.4392 - val_accuracy: 0.8207
Epoch 16/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4023 - accuracy: 0.8200 - val_loss: 0.4477 - val_accuracy: 0.8091
Epoch 17/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4008 - accuracy: 0.8216 - val_loss: 0.4247 - val_accuracy: 0.8219
Epoch 18/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3994 - accuracy: 0.8230 - val_loss: 0.4258 - val_accuracy: 0.8277
Epoch 19/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3959 - accuracy: 0.8242 - val_loss: 0.4223 - val_accuracy: 0.8242
Epoch 20/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.3914 - accuracy: 0.8234 - val_loss: 0.4334 - val_accuracy: 0.8219
Epoch 21/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.3903 - accuracy: 0.8273 - val_loss: 0.4295 - val_accuracy: 0.8289
Epoch 22/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3874 - accuracy: 0.8277 - val_loss: 0.4503 - val_accuracy: 0.8184
Epoch 23/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4070 - accuracy: 0.8195 - val_loss: 0.4360 - val_accuracy: 0.8254
Epoch 24/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.3995 - accuracy: 0.8225 - val_loss: 0.4254 - val_accuracy: 0.8324
Epoch 25/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3869 - accuracy: 0.8270 - val_loss: 0.4352 - val_accuracy: 0.8219
Epoch 26/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.3881 - accuracy: 0.8265 - val_loss: 0.4437 - val_accuracy: 0.8265
Epoch 27/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3796 - accuracy: 0.8299 - val_loss: 0.4478 - val_accuracy: 0.8324
Epoch 28/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3750 - accuracy: 0.8301 - val_loss: 0.4339 - val_accuracy: 0.8289
Epoch 29/30
7724/7724 [==============================] - 2s 204us/sample - loss: 0.3723 - accuracy: 0.8330 - val_loss: 0.4552 - val_accuracy: 0.8126
Epoch 30/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.3714 - accuracy: 0.8321 - val_loss: 0.4331 - val_accuracy: 0.8265
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 3s 381us/sample - loss: 0.5084 - accuracy: 0.7925 - val_loss: 0.4918 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4813 - accuracy: 0.7931 - val_loss: 0.7217 - val_accuracy: 0.7905
Epoch 3/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4973 - accuracy: 0.7931 - val_loss: 0.4896 - val_accuracy: 0.7905
Epoch 4/30
7724/7724 [==============================] - 2s 278us/sample - loss: 0.4708 - accuracy: 0.7931 - val_loss: 0.4783 - val_accuracy: 0.7905
Epoch 5/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4628 - accuracy: 0.7931 - val_loss: 0.4740 - val_accuracy: 0.7905
Epoch 6/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4555 - accuracy: 0.7991 - val_loss: 0.4575 - val_accuracy: 0.7974
Epoch 7/30
7724/7724 [==============================] - 2s 279us/sample - loss: 0.4524 - accuracy: 0.8039 - val_loss: 0.4746 - val_accuracy: 0.8091
Epoch 8/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.4426 - accuracy: 0.8090 - val_loss: 0.5168 - val_accuracy: 0.7974
Epoch 9/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4344 - accuracy: 0.8115 - val_loss: 0.4419 - val_accuracy: 0.8137
Epoch 10/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.4265 - accuracy: 0.8156 - val_loss: 0.4413 - val_accuracy: 0.8184
Epoch 11/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.4194 - accuracy: 0.8189 - val_loss: 0.4369 - val_accuracy: 0.8126
Epoch 12/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4131 - accuracy: 0.8216 - val_loss: 0.4364 - val_accuracy: 0.8289
Epoch 13/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.4070 - accuracy: 0.8215 - val_loss: 0.4411 - val_accuracy: 0.8231
Epoch 14/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.4066 - accuracy: 0.8226 - val_loss: 0.4396 - val_accuracy: 0.8242
Epoch 15/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3990 - accuracy: 0.8265 - val_loss: 0.4371 - val_accuracy: 0.8300
Epoch 16/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3945 - accuracy: 0.8239 - val_loss: 0.4677 - val_accuracy: 0.8277
Epoch 17/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3928 - accuracy: 0.8247 - val_loss: 0.4844 - val_accuracy: 0.8254
Epoch 18/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3824 - accuracy: 0.8283 - val_loss: 0.4426 - val_accuracy: 0.8277
Epoch 19/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.3808 - accuracy: 0.8310 - val_loss: 0.4382 - val_accuracy: 0.8265
Epoch 20/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3743 - accuracy: 0.8314 - val_loss: 0.4928 - val_accuracy: 0.8254
Epoch 21/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3733 - accuracy: 0.8330 - val_loss: 0.4230 - val_accuracy: 0.8219
Epoch 22/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3693 - accuracy: 0.8307 - val_loss: 0.4700 - val_accuracy: 0.8300
Epoch 23/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3636 - accuracy: 0.8351 - val_loss: 0.4553 - val_accuracy: 0.8184
Epoch 24/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.3605 - accuracy: 0.8364 - val_loss: 0.4520 - val_accuracy: 0.8254
Epoch 25/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3598 - accuracy: 0.8383 - val_loss: 0.4397 - val_accuracy: 0.8149
Epoch 26/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.3483 - accuracy: 0.8391 - val_loss: 0.4792 - val_accuracy: 0.8242
Epoch 27/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3418 - accuracy: 0.8419 - val_loss: 0.4585 - val_accuracy: 0.8242
Epoch 28/30
7724/7724 [==============================] - 2s 275us/sample - loss: 0.3397 - accuracy: 0.8422 - val_loss: 0.4581 - val_accuracy: 0.8149
Epoch 29/30
7724/7724 [==============================] - 2s 277us/sample - loss: 0.3345 - accuracy: 0.8437 - val_loss: 0.4708 - val_accuracy: 0.8161
Epoch 30/30
7724/7724 [==============================] - 2s 276us/sample - loss: 0.3285 - accuracy: 0.8493 - val_loss: 0.4696 - val_accuracy: 0.8196
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 2s 283us/sample - loss: 0.5113 - accuracy: 0.7929 - val_loss: 0.4966 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4778 - accuracy: 0.7961 - val_loss: 0.4906 - val_accuracy: 0.7974
Epoch 3/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4675 - accuracy: 0.7965 - val_loss: 0.4992 - val_accuracy: 0.7974
Epoch 4/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4636 - accuracy: 0.7973 - val_loss: 0.4933 - val_accuracy: 0.8079
Epoch 5/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4561 - accuracy: 0.8022 - val_loss: 0.4657 - val_accuracy: 0.8126
Epoch 6/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4504 - accuracy: 0.8050 - val_loss: 0.4563 - val_accuracy: 0.8114
Epoch 7/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4408 - accuracy: 0.8099 - val_loss: 0.4471 - val_accuracy: 0.8172
Epoch 8/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4331 - accuracy: 0.8163 - val_loss: 0.4400 - val_accuracy: 0.8231
Epoch 9/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4298 - accuracy: 0.8150 - val_loss: 0.4276 - val_accuracy: 0.8242
Epoch 10/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4226 - accuracy: 0.8208 - val_loss: 0.4247 - val_accuracy: 0.8242
Epoch 11/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4246 - accuracy: 0.8172 - val_loss: 0.4334 - val_accuracy: 0.8265
Epoch 12/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4201 - accuracy: 0.8203 - val_loss: 0.4243 - val_accuracy: 0.8289
Epoch 13/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4167 - accuracy: 0.8195 - val_loss: 0.4262 - val_accuracy: 0.8231
Epoch 14/30
7724/7724 [==============================] - 1s 172us/sample - loss: 0.4144 - accuracy: 0.8213 - val_loss: 0.4226 - val_accuracy: 0.8242
Epoch 15/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4123 - accuracy: 0.8221 - val_loss: 0.4360 - val_accuracy: 0.8149
Epoch 16/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4085 - accuracy: 0.8225 - val_loss: 0.4297 - val_accuracy: 0.8207
Epoch 17/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4085 - accuracy: 0.8243 - val_loss: 0.4278 - val_accuracy: 0.8254
Epoch 18/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4040 - accuracy: 0.8220 - val_loss: 0.4273 - val_accuracy: 0.8324
Epoch 19/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.4029 - accuracy: 0.8246 - val_loss: 0.4238 - val_accuracy: 0.8312
Epoch 20/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.4000 - accuracy: 0.8273 - val_loss: 0.4211 - val_accuracy: 0.8207
Epoch 21/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3984 - accuracy: 0.8243 - val_loss: 0.4188 - val_accuracy: 0.8219
Epoch 22/30
7724/7724 [==============================] - 1s 173us/sample - loss: 0.3941 - accuracy: 0.8259 - val_loss: 0.4236 - val_accuracy: 0.8254
Epoch 23/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3902 - accuracy: 0.8243 - val_loss: 0.4294 - val_accuracy: 0.8254
Epoch 24/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3896 - accuracy: 0.8261 - val_loss: 0.4344 - val_accuracy: 0.8324
Epoch 25/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.3899 - accuracy: 0.8294 - val_loss: 0.4277 - val_accuracy: 0.8161
Epoch 26/30
7724/7724 [==============================] - 1s 175us/sample - loss: 0.3850 - accuracy: 0.8304 - val_loss: 0.4457 - val_accuracy: 0.8231
Epoch 27/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3870 - accuracy: 0.8277 - val_loss: 0.4532 - val_accuracy: 0.8207
Epoch 28/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3824 - accuracy: 0.8299 - val_loss: 0.4337 - val_accuracy: 0.8277
Epoch 29/30
7724/7724 [==============================] - 1s 174us/sample - loss: 0.3793 - accuracy: 0.8307 - val_loss: 0.4213 - val_accuracy: 0.8254
Epoch 30/30
7724/7724 [==============================] - 1s 176us/sample - loss: 0.3783 - accuracy: 0.8321 - val_loss: 0.4281 - val_accuracy: 0.8219
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 2s 310us/sample - loss: 0.5081 - accuracy: 0.7947 - val_loss: 0.4904 - val_accuracy: 0.7963
Epoch 2/30
7724/7724 [==============================] - 2s 208us/sample - loss: 0.4728 - accuracy: 0.7960 - val_loss: 0.7797 - val_accuracy: 0.7963
Epoch 3/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4833 - accuracy: 0.7961 - val_loss: 0.4794 - val_accuracy: 0.7974
Epoch 4/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4612 - accuracy: 0.8006 - val_loss: 0.4998 - val_accuracy: 0.7963
Epoch 5/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4517 - accuracy: 0.8050 - val_loss: 0.4695 - val_accuracy: 0.8091
Epoch 6/30
7724/7724 [==============================] - 2s 208us/sample - loss: 0.4451 - accuracy: 0.8077 - val_loss: 0.4603 - val_accuracy: 0.8184
Epoch 7/30
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7724/7724 [==============================] - 2s 207us/sample - loss: 0.4421 - accuracy: 0.8107 - val_loss: 0.4609 - val_accuracy: 0.8056
Epoch 8/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.4308 - accuracy: 0.8128 - val_loss: 0.4363 - val_accuracy: 0.8254
Epoch 9/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4241 - accuracy: 0.8140 - val_loss: 0.4334 - val_accuracy: 0.8231
Epoch 10/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.4194 - accuracy: 0.8185 - val_loss: 0.4230 - val_accuracy: 0.8312
Epoch 11/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4139 - accuracy: 0.8228 - val_loss: 0.4313 - val_accuracy: 0.8254
Epoch 12/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.4111 - accuracy: 0.8226 - val_loss: 0.4256 - val_accuracy: 0.8254
Epoch 13/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4078 - accuracy: 0.8202 - val_loss: 0.4275 - val_accuracy: 0.8219
Epoch 14/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4030 - accuracy: 0.8234 - val_loss: 0.4278 - val_accuracy: 0.8300
Epoch 15/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.4059 - accuracy: 0.8219 - val_loss: 0.4304 - val_accuracy: 0.8219
Epoch 16/30
7724/7724 [==============================] - 2s 205us/sample - loss: 0.4030 - accuracy: 0.8243 - val_loss: 0.4491 - val_accuracy: 0.8196
Epoch 17/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3964 - accuracy: 0.8257 - val_loss: 0.4383 - val_accuracy: 0.8335
Epoch 18/30
7724/7724 [==============================] - 2s 208us/sample - loss: 0.3962 - accuracy: 0.8255 - val_loss: 0.4251 - val_accuracy: 0.8254
Epoch 19/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3877 - accuracy: 0.8276 - val_loss: 0.4327 - val_accuracy: 0.8265
Epoch 20/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3862 - accuracy: 0.8272 - val_loss: 0.4434 - val_accuracy: 0.8207
Epoch 21/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3808 - accuracy: 0.8298 - val_loss: 0.4516 - val_accuracy: 0.8277
Epoch 22/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3767 - accuracy: 0.8334 - val_loss: 0.4412 - val_accuracy: 0.8231
Epoch 23/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3761 - accuracy: 0.8327 - val_loss: 0.4386 - val_accuracy: 0.8324
Epoch 24/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3736 - accuracy: 0.8326 - val_loss: 0.4458 - val_accuracy: 0.8335
Epoch 25/30
7724/7724 [==============================] - 2s 208us/sample - loss: 0.3695 - accuracy: 0.8354 - val_loss: 0.4609 - val_accuracy: 0.8300
Epoch 26/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3630 - accuracy: 0.8365 - val_loss: 0.5187 - val_accuracy: 0.8300
Epoch 27/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3583 - accuracy: 0.8364 - val_loss: 0.5793 - val_accuracy: 0.8300
Epoch 28/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3554 - accuracy: 0.8396 - val_loss: 0.5591 - val_accuracy: 0.8126
Epoch 29/30
7724/7724 [==============================] - 2s 206us/sample - loss: 0.3541 - accuracy: 0.8378 - val_loss: 0.5260 - val_accuracy: 0.8359
Epoch 30/30
7724/7724 [==============================] - 2s 207us/sample - loss: 0.3516 - accuracy: 0.8426 - val_loss: 0.5727 - val_accuracy: 0.8207
Train on 7724 samples, validate on 859 samples
Epoch 1/30
7724/7724 [==============================] - 3s 398us/sample - loss: 0.5071 - accuracy: 0.7931 - val_loss: 0.4858 - val_accuracy: 0.7905
Epoch 2/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.4917 - accuracy: 0.7931 - val_loss: 0.4847 - val_accuracy: 0.7905
Epoch 3/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.4957 - accuracy: 0.7931 - val_loss: 0.5019 - val_accuracy: 0.7905
Epoch 4/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4835 - accuracy: 0.7953 - val_loss: 0.4851 - val_accuracy: 0.7963
Epoch 5/30
7724/7724 [==============================] - 2s 293us/sample - loss: 0.4724 - accuracy: 0.7957 - val_loss: 0.4761 - val_accuracy: 0.7963
Epoch 6/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.4624 - accuracy: 0.7958 - val_loss: 0.4920 - val_accuracy: 0.7963
Epoch 7/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4531 - accuracy: 0.8006 - val_loss: 0.4826 - val_accuracy: 0.7963
Epoch 8/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.4473 - accuracy: 0.8074 - val_loss: 0.4725 - val_accuracy: 0.8184
Epoch 9/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.4405 - accuracy: 0.8123 - val_loss: 0.4827 - val_accuracy: 0.7870
Epoch 10/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4355 - accuracy: 0.8149 - val_loss: 0.4439 - val_accuracy: 0.8289
Epoch 11/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4261 - accuracy: 0.8175 - val_loss: 0.4381 - val_accuracy: 0.8300
Epoch 12/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4204 - accuracy: 0.8209 - val_loss: 0.4294 - val_accuracy: 0.8289
Epoch 13/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4207 - accuracy: 0.8189 - val_loss: 0.4392 - val_accuracy: 0.8300
Epoch 14/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.4103 - accuracy: 0.8230 - val_loss: 0.4330 - val_accuracy: 0.8277
Epoch 15/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4071 - accuracy: 0.8226 - val_loss: 0.4502 - val_accuracy: 0.8312
Epoch 16/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4035 - accuracy: 0.8247 - val_loss: 0.4269 - val_accuracy: 0.8289
Epoch 17/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.4020 - accuracy: 0.8222 - val_loss: 0.4523 - val_accuracy: 0.8114
Epoch 18/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.3924 - accuracy: 0.8274 - val_loss: 0.4497 - val_accuracy: 0.8231
Epoch 19/30
7724/7724 [==============================] - 2s 291us/sample - loss: 0.3894 - accuracy: 0.8246 - val_loss: 0.4512 - val_accuracy: 0.8231
Epoch 20/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3874 - accuracy: 0.8256 - val_loss: 0.4372 - val_accuracy: 0.8254
Epoch 21/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3808 - accuracy: 0.8282 - val_loss: 0.4422 - val_accuracy: 0.8300
Epoch 22/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3759 - accuracy: 0.8292 - val_loss: 0.5035 - val_accuracy: 0.8347
Epoch 23/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3717 - accuracy: 0.8322 - val_loss: 0.4557 - val_accuracy: 0.8277
Epoch 24/30
7724/7724 [==============================] - 2s 288us/sample - loss: 0.3671 - accuracy: 0.8318 - val_loss: 0.4423 - val_accuracy: 0.8289
Epoch 25/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.3592 - accuracy: 0.8384 - val_loss: 0.4719 - val_accuracy: 0.8324
Epoch 26/30
7724/7724 [==============================] - 2s 288us/sample - loss: 0.3546 - accuracy: 0.8366 - val_loss: 0.5057 - val_accuracy: 0.8324
Epoch 27/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.3494 - accuracy: 0.8410 - val_loss: 0.4644 - val_accuracy: 0.8312
Epoch 28/30
7724/7724 [==============================] - 2s 290us/sample - loss: 0.3421 - accuracy: 0.8462 - val_loss: 0.5061 - val_accuracy: 0.8312
Epoch 29/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3378 - accuracy: 0.8444 - val_loss: 0.4919 - val_accuracy: 0.8184
Epoch 30/30
7724/7724 [==============================] - 2s 289us/sample - loss: 0.3328 - accuracy: 0.8476 - val_loss: 0.5713 - val_accuracy: 0.8056
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;32&#39;</span><span class="p">,</span><span class="s1">&#39;64&#39;</span><span class="p">,</span> <span class="s1">&#39;128&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_37_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_37_0.png" />
</div>
</div>
</div>
<div class="section" id="cargando-glove-pre-entrenados">
<h2>Cargando GloVe pre-entrenados<a class="headerlink" href="#cargando-glove-pre-entrenados" title="Permalink to this headline">¬∂</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Model_Sentimen</span><span class="p">(</span><span class="n">Embeb</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">Embeb</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">wordsmatrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">LSTM</span><span class="p">(</span><span class="n">cells</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s1">&#39;/home/julian/Documents/Datasets/glove.twitter.27B/&#39;</span>
<span class="k">def</span> <span class="nf">load_embeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
    <span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.twitter.27B.&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;d.txt&#39;</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_g</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word_g</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> word vectors.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">embeddings_index</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_embedding_matrix</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">,</span><span class="n">EMBEDDING_DIM</span><span class="p">):</span>
    <span class="c1">#EMBEDDING_DIM = 25</span>
    <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># words not found in embedding index will be all-zeros.</span>
            <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
    <span class="k">return</span> <span class="n">embedding_matrix</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cells</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
        <span class="c1">#---------------------------------------------------------------------------------</span>
        <span class="n">embeddings_index</span> <span class="o">=</span> <span class="n">load_embeddings</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">load_embedding_matrix</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">wordsmatrix</span> <span class="o">=</span> <span class="n">embedding_matrix</span><span class="p">[:</span><span class="mi">2001</span><span class="p">,:]</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 299us/sample - loss: 0.3930 - accuracy: 0.8316 - val_loss: 0.3240 - val_accuracy: 0.8788
Epoch 2/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2992 - accuracy: 0.8823 - val_loss: 0.2969 - val_accuracy: 0.8842
Epoch 3/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2787 - accuracy: 0.8871 - val_loss: 0.2871 - val_accuracy: 0.8842
Epoch 4/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2681 - accuracy: 0.8917 - val_loss: 0.2838 - val_accuracy: 0.8842
Epoch 5/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2550 - accuracy: 0.8965 - val_loss: 0.2867 - val_accuracy: 0.8929
Epoch 6/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2527 - accuracy: 0.8988 - val_loss: 0.2809 - val_accuracy: 0.8896
Epoch 7/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2430 - accuracy: 0.9007 - val_loss: 0.2819 - val_accuracy: 0.8929
Epoch 8/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2401 - accuracy: 0.9011 - val_loss: 0.2763 - val_accuracy: 0.8907
Epoch 9/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2311 - accuracy: 0.9044 - val_loss: 0.2803 - val_accuracy: 0.8939
Epoch 10/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2267 - accuracy: 0.9047 - val_loss: 0.2786 - val_accuracy: 0.8929
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 296us/sample - loss: 0.3871 - accuracy: 0.8145 - val_loss: 0.3356 - val_accuracy: 0.8723
Epoch 2/10
8308/8308 [==============================] - 2s 194us/sample - loss: 0.3157 - accuracy: 0.8837 - val_loss: 0.3191 - val_accuracy: 0.8896
Epoch 3/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2958 - accuracy: 0.8956 - val_loss: 0.3144 - val_accuracy: 0.8918
Epoch 4/10
8308/8308 [==============================] - 2s 194us/sample - loss: 0.2823 - accuracy: 0.8989 - val_loss: 0.2973 - val_accuracy: 0.8961
Epoch 5/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2690 - accuracy: 0.9053 - val_loss: 0.3091 - val_accuracy: 0.8799
Epoch 6/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2557 - accuracy: 0.9070 - val_loss: 0.3000 - val_accuracy: 0.8918
Epoch 7/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2503 - accuracy: 0.9095 - val_loss: 0.2924 - val_accuracy: 0.8983
Epoch 8/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2393 - accuracy: 0.9145 - val_loss: 0.3052 - val_accuracy: 0.8918
Epoch 9/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2312 - accuracy: 0.9133 - val_loss: 0.2968 - val_accuracy: 0.8994
Epoch 10/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2215 - accuracy: 0.9153 - val_loss: 0.2989 - val_accuracy: 0.8918
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 373us/sample - loss: 0.6613 - accuracy: 0.7897 - val_loss: 0.6300 - val_accuracy: 0.7976
Epoch 2/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.4554 - accuracy: 0.8584 - val_loss: 0.3090 - val_accuracy: 0.8788
Epoch 3/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.3180 - accuracy: 0.8950 - val_loss: 0.2866 - val_accuracy: 0.8929
Epoch 4/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.2889 - accuracy: 0.9025 - val_loss: 0.2862 - val_accuracy: 0.8950
Epoch 5/10
8308/8308 [==============================] - 2s 273us/sample - loss: 0.2713 - accuracy: 0.9048 - val_loss: 0.2750 - val_accuracy: 0.8874
Epoch 6/10
8308/8308 [==============================] - 2s 273us/sample - loss: 0.2609 - accuracy: 0.9101 - val_loss: 0.2802 - val_accuracy: 0.8896
Epoch 7/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.2462 - accuracy: 0.9135 - val_loss: 0.2816 - val_accuracy: 0.8831
Epoch 8/10
8308/8308 [==============================] - 2s 272us/sample - loss: 0.2330 - accuracy: 0.9156 - val_loss: 0.2832 - val_accuracy: 0.8907
Epoch 9/10
8308/8308 [==============================] - 2s 272us/sample - loss: 0.2208 - accuracy: 0.9227 - val_loss: 0.2957 - val_accuracy: 0.8896
Epoch 10/10
8308/8308 [==============================] - 2s 273us/sample - loss: 0.2068 - accuracy: 0.9279 - val_loss: 0.3105 - val_accuracy: 0.8907
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 283us/sample - loss: 0.3670 - accuracy: 0.8436 - val_loss: 0.2895 - val_accuracy: 0.8918
Epoch 2/10
8308/8308 [==============================] - 1s 177us/sample - loss: 0.2790 - accuracy: 0.8882 - val_loss: 0.2850 - val_accuracy: 0.8939
Epoch 3/10
8308/8308 [==============================] - 1s 177us/sample - loss: 0.2556 - accuracy: 0.9021 - val_loss: 0.2572 - val_accuracy: 0.9048
Epoch 4/10
8308/8308 [==============================] - 1s 175us/sample - loss: 0.2396 - accuracy: 0.9072 - val_loss: 0.2522 - val_accuracy: 0.9037
Epoch 5/10
8308/8308 [==============================] - 1s 173us/sample - loss: 0.2246 - accuracy: 0.9138 - val_loss: 0.2516 - val_accuracy: 0.9037
Epoch 6/10
8308/8308 [==============================] - 1s 174us/sample - loss: 0.2118 - accuracy: 0.9178 - val_loss: 0.2532 - val_accuracy: 0.9048
Epoch 7/10
8308/8308 [==============================] - 1s 173us/sample - loss: 0.2058 - accuracy: 0.9226 - val_loss: 0.2581 - val_accuracy: 0.9015
Epoch 8/10
8308/8308 [==============================] - 1s 174us/sample - loss: 0.1964 - accuracy: 0.9245 - val_loss: 0.2664 - val_accuracy: 0.9058
Epoch 9/10
8308/8308 [==============================] - 1s 174us/sample - loss: 0.1870 - accuracy: 0.9310 - val_loss: 0.2453 - val_accuracy: 0.9026
Epoch 10/10
8308/8308 [==============================] - 1s 173us/sample - loss: 0.1791 - accuracy: 0.9334 - val_loss: 0.2598 - val_accuracy: 0.9048
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 303us/sample - loss: 0.3489 - accuracy: 0.8580 - val_loss: 0.2869 - val_accuracy: 0.8939
Epoch 2/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.2712 - accuracy: 0.8942 - val_loss: 0.2669 - val_accuracy: 0.8929
Epoch 3/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2554 - accuracy: 0.9043 - val_loss: 0.2580 - val_accuracy: 0.8972
Epoch 4/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2358 - accuracy: 0.9141 - val_loss: 0.2638 - val_accuracy: 0.8994
Epoch 5/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2206 - accuracy: 0.9171 - val_loss: 0.2616 - val_accuracy: 0.9004
Epoch 6/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2064 - accuracy: 0.9261 - val_loss: 0.2607 - val_accuracy: 0.8961
Epoch 7/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1961 - accuracy: 0.9265 - val_loss: 0.2676 - val_accuracy: 0.8983
Epoch 8/10
8308/8308 [==============================] - 2s 198us/sample - loss: 0.1822 - accuracy: 0.9367 - val_loss: 0.3059 - val_accuracy: 0.9026
Epoch 9/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1680 - accuracy: 0.9396 - val_loss: 0.2778 - val_accuracy: 0.8994
Epoch 10/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1543 - accuracy: 0.9427 - val_loss: 0.3181 - val_accuracy: 0.9004
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 377us/sample - loss: 0.3607 - accuracy: 0.8426 - val_loss: 0.2723 - val_accuracy: 0.8972
Epoch 2/10
8308/8308 [==============================] - 2s 272us/sample - loss: 0.2860 - accuracy: 0.8997 - val_loss: 0.2579 - val_accuracy: 0.8972
Epoch 3/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.2452 - accuracy: 0.9095 - val_loss: 0.2545 - val_accuracy: 0.8983
Epoch 4/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.2228 - accuracy: 0.9167 - val_loss: 0.2484 - val_accuracy: 0.8972
Epoch 5/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.2179 - accuracy: 0.9188 - val_loss: 0.2571 - val_accuracy: 0.9026
Epoch 6/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.2058 - accuracy: 0.9239 - val_loss: 0.2612 - val_accuracy: 0.8994
Epoch 7/10
8308/8308 [==============================] - 2s 271us/sample - loss: 0.1847 - accuracy: 0.9310 - val_loss: 0.2490 - val_accuracy: 0.9048
Epoch 8/10
8308/8308 [==============================] - 2s 272us/sample - loss: 0.1678 - accuracy: 0.9369 - val_loss: 0.2802 - val_accuracy: 0.9015
Epoch 9/10
8308/8308 [==============================] - 2s 273us/sample - loss: 0.1607 - accuracy: 0.9393 - val_loss: 0.2831 - val_accuracy: 0.9015
Epoch 10/10
8308/8308 [==============================] - 2s 274us/sample - loss: 0.1419 - accuracy: 0.9468 - val_loss: 0.3975 - val_accuracy: 0.8961
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 279us/sample - loss: 0.3614 - accuracy: 0.8471 - val_loss: 0.3064 - val_accuracy: 0.8874
Epoch 2/10
8308/8308 [==============================] - 1s 173us/sample - loss: 0.2617 - accuracy: 0.8938 - val_loss: 0.2561 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.2345 - accuracy: 0.9008 - val_loss: 0.2557 - val_accuracy: 0.8961
Epoch 4/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2161 - accuracy: 0.9083 - val_loss: 0.2764 - val_accuracy: 0.8950
Epoch 5/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2023 - accuracy: 0.9097 - val_loss: 0.2684 - val_accuracy: 0.9026
Epoch 6/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1897 - accuracy: 0.9231 - val_loss: 0.2553 - val_accuracy: 0.8994
Epoch 7/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1785 - accuracy: 0.9320 - val_loss: 0.3025 - val_accuracy: 0.9069
Epoch 8/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1636 - accuracy: 0.9356 - val_loss: 0.2942 - val_accuracy: 0.9015
Epoch 9/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1507 - accuracy: 0.9417 - val_loss: 0.2959 - val_accuracy: 0.8972
Epoch 10/10
8308/8308 [==============================] - 1s 175us/sample - loss: 0.1360 - accuracy: 0.9478 - val_loss: 0.3359 - val_accuracy: 0.8950
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 319us/sample - loss: 0.3339 - accuracy: 0.8656 - val_loss: 0.2702 - val_accuracy: 0.8950
Epoch 2/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.2358 - accuracy: 0.9090 - val_loss: 0.2876 - val_accuracy: 0.8885
Epoch 3/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.2149 - accuracy: 0.9135 - val_loss: 0.2669 - val_accuracy: 0.8950
Epoch 4/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1969 - accuracy: 0.9192 - val_loss: 0.2693 - val_accuracy: 0.8961
Epoch 5/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1845 - accuracy: 0.9271 - val_loss: 0.2692 - val_accuracy: 0.8994
Epoch 6/10
8308/8308 [==============================] - 2s 198us/sample - loss: 0.1700 - accuracy: 0.9339 - val_loss: 0.2884 - val_accuracy: 0.8983
Epoch 7/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1548 - accuracy: 0.9370 - val_loss: 0.3637 - val_accuracy: 0.8939
Epoch 8/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.1405 - accuracy: 0.9442 - val_loss: 0.3375 - val_accuracy: 0.8842
Epoch 9/10
8308/8308 [==============================] - 2s 200us/sample - loss: 0.1252 - accuracy: 0.9496 - val_loss: 0.4892 - val_accuracy: 0.8950
Epoch 10/10
8308/8308 [==============================] - 2s 198us/sample - loss: 0.1126 - accuracy: 0.9578 - val_loss: 0.5413 - val_accuracy: 0.8950
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 380us/sample - loss: 0.3700 - accuracy: 0.8624 - val_loss: 0.2864 - val_accuracy: 0.8885
Epoch 2/10
8308/8308 [==============================] - 2s 281us/sample - loss: 0.2565 - accuracy: 0.9072 - val_loss: 0.2565 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 2s 283us/sample - loss: 0.2226 - accuracy: 0.9194 - val_loss: 0.2581 - val_accuracy: 0.9004
Epoch 4/10
8308/8308 [==============================] - 2s 278us/sample - loss: 0.2073 - accuracy: 0.9274 - val_loss: 0.2595 - val_accuracy: 0.8994
Epoch 5/10
8308/8308 [==============================] - 2s 280us/sample - loss: 0.1872 - accuracy: 0.9312 - val_loss: 0.3262 - val_accuracy: 0.8972
Epoch 6/10
8308/8308 [==============================] - 2s 278us/sample - loss: 0.1780 - accuracy: 0.9380 - val_loss: 0.2411 - val_accuracy: 0.9058
Epoch 7/10
8308/8308 [==============================] - 2s 286us/sample - loss: 0.1765 - accuracy: 0.9408 - val_loss: 0.2841 - val_accuracy: 0.9015
Epoch 8/10
8308/8308 [==============================] - 2s 292us/sample - loss: 0.1437 - accuracy: 0.9514 - val_loss: 0.3131 - val_accuracy: 0.8983
Epoch 9/10
8308/8308 [==============================] - 2s 282us/sample - loss: 0.1265 - accuracy: 0.9578 - val_loss: 0.3730 - val_accuracy: 0.9058
Epoch 10/10
8308/8308 [==============================] - 2s 277us/sample - loss: 0.1099 - accuracy: 0.9635 - val_loss: 0.4473 - val_accuracy: 0.9004
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy= </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_44_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_44_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy= 0.9081853616284106
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s1">&#39;/home/julian/Documents/Datasets/glove.6B/&#39;</span>
<span class="k">def</span> <span class="nf">load_embeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
    <span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.6B.&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;d.txt&#39;</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_g</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word_g</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> word vectors.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">embeddings_index</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">cells</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">]):</span>
        <span class="c1">#---------------------------------------------------------------------------------</span>
        <span class="n">embeddings_index</span> <span class="o">=</span> <span class="n">load_embeddings</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">load_embedding_matrix</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">wordsmatrix</span> <span class="o">=</span> <span class="n">embedding_matrix</span><span class="p">[:</span><span class="mi">2001</span><span class="p">,:]</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">cells</span><span class="p">,</span><span class="n">wordsmatrix</span><span class="p">)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 278us/sample - loss: 0.3986 - accuracy: 0.8346 - val_loss: 0.3231 - val_accuracy: 0.8755
Epoch 2/10
8308/8308 [==============================] - 1s 172us/sample - loss: 0.3000 - accuracy: 0.8805 - val_loss: 0.2968 - val_accuracy: 0.8820
Epoch 3/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.2786 - accuracy: 0.8852 - val_loss: 0.2877 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.2606 - accuracy: 0.8942 - val_loss: 0.2873 - val_accuracy: 0.8918
Epoch 5/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2489 - accuracy: 0.8987 - val_loss: 0.2820 - val_accuracy: 0.8918
Epoch 6/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2351 - accuracy: 0.9018 - val_loss: 0.2891 - val_accuracy: 0.8929
Epoch 7/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2250 - accuracy: 0.9029 - val_loss: 0.3005 - val_accuracy: 0.8896
Epoch 8/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.2100 - accuracy: 0.9149 - val_loss: 0.3121 - val_accuracy: 0.8874
Epoch 9/10
8308/8308 [==============================] - 1s 169us/sample - loss: 0.1987 - accuracy: 0.9207 - val_loss: 0.3026 - val_accuracy: 0.8885
Epoch 10/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1893 - accuracy: 0.9234 - val_loss: 0.3697 - val_accuracy: 0.8853
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 319us/sample - loss: 0.4064 - accuracy: 0.8270 - val_loss: 0.3135 - val_accuracy: 0.8810
Epoch 2/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2848 - accuracy: 0.8887 - val_loss: 0.3084 - val_accuracy: 0.8810
Epoch 3/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2618 - accuracy: 0.8993 - val_loss: 0.2935 - val_accuracy: 0.8874
Epoch 4/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2374 - accuracy: 0.9024 - val_loss: 0.2817 - val_accuracy: 0.8874
Epoch 5/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2228 - accuracy: 0.9090 - val_loss: 0.3002 - val_accuracy: 0.8853
Epoch 6/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.2088 - accuracy: 0.9162 - val_loss: 0.3175 - val_accuracy: 0.8896
Epoch 7/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1925 - accuracy: 0.9233 - val_loss: 0.3481 - val_accuracy: 0.8885
Epoch 8/10
8308/8308 [==============================] - 2s 199us/sample - loss: 0.1838 - accuracy: 0.9271 - val_loss: 0.3014 - val_accuracy: 0.8896
Epoch 9/10
8308/8308 [==============================] - 2s 197us/sample - loss: 0.1709 - accuracy: 0.9319 - val_loss: 0.3457 - val_accuracy: 0.8907
Epoch 10/10
8308/8308 [==============================] - 2s 201us/sample - loss: 0.1548 - accuracy: 0.9368 - val_loss: 0.3851 - val_accuracy: 0.8885
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 374us/sample - loss: 0.4167 - accuracy: 0.8251 - val_loss: 0.3099 - val_accuracy: 0.8777
Epoch 2/10
8308/8308 [==============================] - 2s 269us/sample - loss: 0.3062 - accuracy: 0.8835 - val_loss: 0.2979 - val_accuracy: 0.8820
Epoch 3/10
8308/8308 [==============================] - 2s 269us/sample - loss: 0.2688 - accuracy: 0.9008 - val_loss: 0.2895 - val_accuracy: 0.8896
Epoch 4/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.2489 - accuracy: 0.9085 - val_loss: 0.2851 - val_accuracy: 0.8864
Epoch 5/10
8308/8308 [==============================] - 2s 275us/sample - loss: 0.2303 - accuracy: 0.9148 - val_loss: 0.2952 - val_accuracy: 0.8874
Epoch 6/10
8308/8308 [==============================] - 2s 274us/sample - loss: 0.2096 - accuracy: 0.9200 - val_loss: 0.3050 - val_accuracy: 0.8896
Epoch 7/10
8308/8308 [==============================] - 2s 266us/sample - loss: 0.1960 - accuracy: 0.9277 - val_loss: 0.3356 - val_accuracy: 0.8853
Epoch 8/10
8308/8308 [==============================] - 2s 266us/sample - loss: 0.1747 - accuracy: 0.9356 - val_loss: 0.3122 - val_accuracy: 0.8983
Epoch 9/10
8308/8308 [==============================] - 2s 267us/sample - loss: 0.1557 - accuracy: 0.9421 - val_loss: 0.3750 - val_accuracy: 0.8896
Epoch 10/10
8308/8308 [==============================] - 2s 270us/sample - loss: 0.1354 - accuracy: 0.9509 - val_loss: 0.3869 - val_accuracy: 0.8896
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 289us/sample - loss: 0.3764 - accuracy: 0.8412 - val_loss: 0.2974 - val_accuracy: 0.8831
Epoch 2/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.2710 - accuracy: 0.8923 - val_loss: 0.2782 - val_accuracy: 0.8853
Epoch 3/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.2446 - accuracy: 0.9031 - val_loss: 0.2747 - val_accuracy: 0.9015
Epoch 4/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.2292 - accuracy: 0.9102 - val_loss: 0.2734 - val_accuracy: 0.9015
Epoch 5/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.2150 - accuracy: 0.9180 - val_loss: 0.2720 - val_accuracy: 0.8972
Epoch 6/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.1963 - accuracy: 0.9227 - val_loss: 0.2825 - val_accuracy: 0.9004
Epoch 7/10
8308/8308 [==============================] - 1s 170us/sample - loss: 0.1829 - accuracy: 0.9295 - val_loss: 0.3325 - val_accuracy: 0.8961
Epoch 8/10
8308/8308 [==============================] - 1s 171us/sample - loss: 0.1705 - accuracy: 0.9351 - val_loss: 0.3106 - val_accuracy: 0.8777
Epoch 9/10
8308/8308 [==============================] - 1s 168us/sample - loss: 0.1578 - accuracy: 0.9384 - val_loss: 0.3065 - val_accuracy: 0.8950
Epoch 10/10
8308/8308 [==============================] - 1s 167us/sample - loss: 0.1493 - accuracy: 0.9409 - val_loss: 0.3171 - val_accuracy: 0.9004
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 303us/sample - loss: 0.3609 - accuracy: 0.8519 - val_loss: 0.2977 - val_accuracy: 0.8896
Epoch 2/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.2626 - accuracy: 0.9021 - val_loss: 0.2749 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.2380 - accuracy: 0.9100 - val_loss: 0.2699 - val_accuracy: 0.9004
Epoch 4/10
8308/8308 [==============================] - 2s 198us/sample - loss: 0.2181 - accuracy: 0.9189 - val_loss: 0.2731 - val_accuracy: 0.9037
Epoch 5/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1983 - accuracy: 0.9244 - val_loss: 0.2902 - val_accuracy: 0.9037
Epoch 6/10
8308/8308 [==============================] - 2s 193us/sample - loss: 0.1772 - accuracy: 0.9302 - val_loss: 0.2687 - val_accuracy: 0.9037
Epoch 7/10
8308/8308 [==============================] - 2s 204us/sample - loss: 0.1619 - accuracy: 0.9405 - val_loss: 0.2827 - val_accuracy: 0.9015
Epoch 8/10
8308/8308 [==============================] - 2s 201us/sample - loss: 0.1444 - accuracy: 0.9466 - val_loss: 0.3160 - val_accuracy: 0.8939
Epoch 9/10
8308/8308 [==============================] - 2s 196us/sample - loss: 0.1343 - accuracy: 0.9505 - val_loss: 0.3784 - val_accuracy: 0.9048
Epoch 10/10
8308/8308 [==============================] - 2s 195us/sample - loss: 0.1396 - accuracy: 0.9476 - val_loss: 0.3325 - val_accuracy: 0.9015
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 403us/sample - loss: 0.3688 - accuracy: 0.8615 - val_loss: 0.2903 - val_accuracy: 0.8918
Epoch 2/10
8308/8308 [==============================] - 2s 276us/sample - loss: 0.2704 - accuracy: 0.9041 - val_loss: 0.2656 - val_accuracy: 0.8864
Epoch 3/10
8308/8308 [==============================] - 2s 276us/sample - loss: 0.2355 - accuracy: 0.9123 - val_loss: 0.2673 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 2s 275us/sample - loss: 0.2139 - accuracy: 0.9231 - val_loss: 0.2824 - val_accuracy: 0.8907
Epoch 5/10
8308/8308 [==============================] - 2s 276us/sample - loss: 0.1920 - accuracy: 0.9313 - val_loss: 0.2672 - val_accuracy: 0.8994
Epoch 6/10
8308/8308 [==============================] - 2s 284us/sample - loss: 0.1747 - accuracy: 0.9387 - val_loss: 0.3165 - val_accuracy: 0.8994
Epoch 7/10
8308/8308 [==============================] - 2s 282us/sample - loss: 0.1650 - accuracy: 0.9425 - val_loss: 0.2842 - val_accuracy: 0.8929
Epoch 8/10
8308/8308 [==============================] - 2s 279us/sample - loss: 0.1459 - accuracy: 0.9496 - val_loss: 0.3445 - val_accuracy: 0.8972
Epoch 9/10
8308/8308 [==============================] - 2s 279us/sample - loss: 0.1297 - accuracy: 0.9549 - val_loss: 0.3672 - val_accuracy: 0.8950
Epoch 10/10
8308/8308 [==============================] - 2s 275us/sample - loss: 0.1178 - accuracy: 0.9634 - val_loss: 0.2982 - val_accuracy: 0.8831
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 2s 282us/sample - loss: 0.3811 - accuracy: 0.8493 - val_loss: 0.2746 - val_accuracy: 0.8961
Epoch 2/10
8308/8308 [==============================] - 1s 177us/sample - loss: 0.2652 - accuracy: 0.9037 - val_loss: 0.2702 - val_accuracy: 0.9015
Epoch 3/10
8308/8308 [==============================] - 1s 180us/sample - loss: 0.2428 - accuracy: 0.9102 - val_loss: 0.2714 - val_accuracy: 0.9004
Epoch 4/10
8308/8308 [==============================] - 1s 178us/sample - loss: 0.2196 - accuracy: 0.9194 - val_loss: 0.2598 - val_accuracy: 0.9026
Epoch 5/10
8308/8308 [==============================] - 1s 178us/sample - loss: 0.2019 - accuracy: 0.9249 - val_loss: 0.2624 - val_accuracy: 0.9004
Epoch 6/10
8308/8308 [==============================] - 1s 177us/sample - loss: 0.1856 - accuracy: 0.9331 - val_loss: 0.2817 - val_accuracy: 0.9091
Epoch 7/10
8308/8308 [==============================] - 1s 175us/sample - loss: 0.1686 - accuracy: 0.9374 - val_loss: 0.2931 - val_accuracy: 0.9037
Epoch 8/10
8308/8308 [==============================] - 1s 178us/sample - loss: 0.1560 - accuracy: 0.9415 - val_loss: 0.3037 - val_accuracy: 0.8972
Epoch 9/10
8308/8308 [==============================] - 1s 177us/sample - loss: 0.1341 - accuracy: 0.9491 - val_loss: 0.3479 - val_accuracy: 0.8972
Epoch 10/10
8308/8308 [==============================] - 1s 179us/sample - loss: 0.1256 - accuracy: 0.9550 - val_loss: 0.4226 - val_accuracy: 0.8983
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 3s 314us/sample - loss: 0.3357 - accuracy: 0.8640 - val_loss: 0.2802 - val_accuracy: 0.8939
Epoch 2/10
8308/8308 [==============================] - 2s 208us/sample - loss: 0.2431 - accuracy: 0.9065 - val_loss: 0.2908 - val_accuracy: 0.8896
Epoch 3/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.2145 - accuracy: 0.9169 - val_loss: 0.2551 - val_accuracy: 0.9037
Epoch 4/10
8308/8308 [==============================] - 2s 211us/sample - loss: 0.1914 - accuracy: 0.9247 - val_loss: 0.2610 - val_accuracy: 0.8972
Epoch 5/10
8308/8308 [==============================] - 2s 209us/sample - loss: 0.1708 - accuracy: 0.9324 - val_loss: 0.2879 - val_accuracy: 0.8994
Epoch 6/10
8308/8308 [==============================] - 2s 207us/sample - loss: 0.1594 - accuracy: 0.9375 - val_loss: 0.2865 - val_accuracy: 0.9037
Epoch 7/10
8308/8308 [==============================] - 2s 206us/sample - loss: 0.1365 - accuracy: 0.9463 - val_loss: 0.3078 - val_accuracy: 0.8961
Epoch 8/10
8308/8308 [==============================] - 2s 208us/sample - loss: 0.1208 - accuracy: 0.9529 - val_loss: 0.3344 - val_accuracy: 0.8929
Epoch 9/10
8308/8308 [==============================] - 2s 213us/sample - loss: 0.1010 - accuracy: 0.9596 - val_loss: 0.4064 - val_accuracy: 0.8994
Epoch 10/10
8308/8308 [==============================] - 2s 211us/sample - loss: 0.0834 - accuracy: 0.9658 - val_loss: 0.3715 - val_accuracy: 0.8939
Found 400000 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 4s 444us/sample - loss: 0.3296 - accuracy: 0.8729 - val_loss: 0.2714 - val_accuracy: 0.8918
Epoch 2/10
8308/8308 [==============================] - 2s 299us/sample - loss: 0.2370 - accuracy: 0.9108 - val_loss: 0.2610 - val_accuracy: 0.8983
Epoch 3/10
8308/8308 [==============================] - 3s 302us/sample - loss: 0.2009 - accuracy: 0.9228 - val_loss: 0.2764 - val_accuracy: 0.9026
Epoch 4/10
8308/8308 [==============================] - 3s 305us/sample - loss: 0.1782 - accuracy: 0.9339 - val_loss: 0.2588 - val_accuracy: 0.9015
Epoch 5/10
8308/8308 [==============================] - 3s 307us/sample - loss: 0.1521 - accuracy: 0.9434 - val_loss: 0.2947 - val_accuracy: 0.8961
Epoch 6/10
8308/8308 [==============================] - 2s 298us/sample - loss: 0.1379 - accuracy: 0.9496 - val_loss: 0.3760 - val_accuracy: 0.9004
Epoch 7/10
8308/8308 [==============================] - 3s 307us/sample - loss: 0.1095 - accuracy: 0.9597 - val_loss: 0.3259 - val_accuracy: 0.8907
Epoch 8/10
8308/8308 [==============================] - 3s 309us/sample - loss: 0.0887 - accuracy: 0.9668 - val_loss: 0.4102 - val_accuracy: 0.8918
Epoch 9/10
8308/8308 [==============================] - 2s 300us/sample - loss: 0.0786 - accuracy: 0.9714 - val_loss: 0.4792 - val_accuracy: 0.8874
Epoch 10/10
8308/8308 [==============================] - 3s 304us/sample - loss: 0.0636 - accuracy: 0.9776 - val_loss: 0.3750 - val_accuracy: 0.8864
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">,</span> <span class="s1">&#39;200&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">,</span> <span class="s1">&#39;200&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">,</span> <span class="s1">&#39;200&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy= </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_47_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_47_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy= 0.9086184495452577
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-4">
<h1>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">¬∂</a></h1>
<p>Use a CNN architeture instead of the LSTM one, combine CNN layers for bigrams and trigrams. You can also use either the keras Embedding Layer or the CBOW embedding matrix. Compare the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s1">&#39;/home/julian/Documents/Datasets/glove.twitter.27B/&#39;</span>
<span class="k">def</span> <span class="nf">load_embeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
    <span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.twitter.27B.&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;d.txt&#39;</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">word_g</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word_g</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> word vectors.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">embeddings_index</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Model_Sentimen</span><span class="p">(</span><span class="n">Embeb</span><span class="p">,</span><span class="n">filters</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="c1">#Embedding(input_dim=max_fatures, output_dim=Embeb, weights = [np.r_[np.zeros((1,Embeb)),wordsmatrix]], trainable = False, mask_zero=True),</span>
        <span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">Embeb</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">wordsmatrix</span><span class="p">],</span> <span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">),</span>
        <span class="n">GlobalMaxPooling1D</span><span class="p">(),</span>
        <span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">),</span>
        <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sensitivity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">especificity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span><span class="n">filters</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">24</span><span class="p">]):</span>
        <span class="c1">#---------------------------------------------------------------------------------</span>
        <span class="n">embeddings_index</span> <span class="o">=</span> <span class="n">load_embeddings</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">load_embedding_matrix</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">,</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">wordsmatrix</span> <span class="o">=</span> <span class="n">embedding_matrix</span><span class="p">[:</span><span class="mi">2001</span><span class="p">,:]</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="c1">#-------------------------------------------------------------------------------------</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model_Sentimen</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">filters</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">))</span>
        <span class="n">sensitivity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">especificity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">especi_score</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>   
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 69us/sample - loss: 0.4596 - accuracy: 0.8067 - val_loss: 0.3438 - val_accuracy: 0.8690
Epoch 2/10
8308/8308 [==============================] - 0s 31us/sample - loss: 0.3176 - accuracy: 0.8742 - val_loss: 0.3125 - val_accuracy: 0.8734
Epoch 3/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.2846 - accuracy: 0.8852 - val_loss: 0.3197 - val_accuracy: 0.8669
Epoch 4/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2741 - accuracy: 0.8907 - val_loss: 0.3025 - val_accuracy: 0.8777
Epoch 5/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.2629 - accuracy: 0.8943 - val_loss: 0.3024 - val_accuracy: 0.8745
Epoch 6/10
8308/8308 [==============================] - 0s 35us/sample - loss: 0.2625 - accuracy: 0.8956 - val_loss: 0.2982 - val_accuracy: 0.8831
Epoch 7/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.2547 - accuracy: 0.8995 - val_loss: 0.3003 - val_accuracy: 0.8788
Epoch 8/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.2538 - accuracy: 0.8985 - val_loss: 0.3012 - val_accuracy: 0.8755
Epoch 9/10
8308/8308 [==============================] - 0s 46us/sample - loss: 0.2478 - accuracy: 0.9029 - val_loss: 0.3028 - val_accuracy: 0.8745
Epoch 10/10
8308/8308 [==============================] - 0s 43us/sample - loss: 0.2464 - accuracy: 0.9030 - val_loss: 0.3056 - val_accuracy: 0.8701
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 64us/sample - loss: 0.4223 - accuracy: 0.8232 - val_loss: 0.3232 - val_accuracy: 0.8745
Epoch 2/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2940 - accuracy: 0.8800 - val_loss: 0.3064 - val_accuracy: 0.8723
Epoch 3/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2694 - accuracy: 0.8908 - val_loss: 0.3001 - val_accuracy: 0.8766
Epoch 4/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2605 - accuracy: 0.8965 - val_loss: 0.3013 - val_accuracy: 0.8755
Epoch 5/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2491 - accuracy: 0.9008 - val_loss: 0.2953 - val_accuracy: 0.8777
Epoch 6/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2425 - accuracy: 0.9023 - val_loss: 0.2989 - val_accuracy: 0.8766
Epoch 7/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2370 - accuracy: 0.9068 - val_loss: 0.2974 - val_accuracy: 0.8777
Epoch 8/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2306 - accuracy: 0.9091 - val_loss: 0.3045 - val_accuracy: 0.8799
Epoch 9/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2279 - accuracy: 0.9086 - val_loss: 0.3028 - val_accuracy: 0.8734
Epoch 10/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2255 - accuracy: 0.9123 - val_loss: 0.3072 - val_accuracy: 0.8766
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 66us/sample - loss: 0.3910 - accuracy: 0.8411 - val_loss: 0.3093 - val_accuracy: 0.8734
Epoch 2/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2758 - accuracy: 0.8891 - val_loss: 0.2991 - val_accuracy: 0.8745
Epoch 3/10
8308/8308 [==============================] - 0s 28us/sample - loss: 0.2580 - accuracy: 0.8952 - val_loss: 0.2921 - val_accuracy: 0.8799
Epoch 4/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2443 - accuracy: 0.9015 - val_loss: 0.2945 - val_accuracy: 0.8788
Epoch 5/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2335 - accuracy: 0.9091 - val_loss: 0.3005 - val_accuracy: 0.8799
Epoch 6/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2257 - accuracy: 0.9117 - val_loss: 0.2924 - val_accuracy: 0.8820
Epoch 7/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2171 - accuracy: 0.9151 - val_loss: 0.3017 - val_accuracy: 0.8690
Epoch 8/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2105 - accuracy: 0.9177 - val_loss: 0.3064 - val_accuracy: 0.8842
Epoch 9/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2067 - accuracy: 0.9180 - val_loss: 0.3099 - val_accuracy: 0.8647
Epoch 10/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2007 - accuracy: 0.9230 - val_loss: 0.3141 - val_accuracy: 0.8669
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 64us/sample - loss: 0.4605 - accuracy: 0.7959 - val_loss: 0.3703 - val_accuracy: 0.8496
Epoch 2/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.3429 - accuracy: 0.8639 - val_loss: 0.3455 - val_accuracy: 0.8571
Epoch 3/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.3162 - accuracy: 0.8741 - val_loss: 0.3389 - val_accuracy: 0.8615
Epoch 4/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2978 - accuracy: 0.8814 - val_loss: 0.3365 - val_accuracy: 0.8561
Epoch 5/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2898 - accuracy: 0.8873 - val_loss: 0.3351 - val_accuracy: 0.8571
Epoch 6/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2816 - accuracy: 0.8877 - val_loss: 0.3334 - val_accuracy: 0.8604
Epoch 7/10
8308/8308 [==============================] - 0s 36us/sample - loss: 0.2739 - accuracy: 0.8903 - val_loss: 0.3326 - val_accuracy: 0.8561
Epoch 8/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2697 - accuracy: 0.8970 - val_loss: 0.3345 - val_accuracy: 0.8561
Epoch 9/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2658 - accuracy: 0.8988 - val_loss: 0.3339 - val_accuracy: 0.8582
Epoch 10/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.2602 - accuracy: 0.8965 - val_loss: 0.3408 - val_accuracy: 0.8506
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 64us/sample - loss: 0.4374 - accuracy: 0.8063 - val_loss: 0.2950 - val_accuracy: 0.8820
Epoch 2/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2631 - accuracy: 0.8923 - val_loss: 0.2724 - val_accuracy: 0.8907
Epoch 3/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2367 - accuracy: 0.9038 - val_loss: 0.2688 - val_accuracy: 0.8918
Epoch 4/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2181 - accuracy: 0.9119 - val_loss: 0.2703 - val_accuracy: 0.8961
Epoch 5/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2113 - accuracy: 0.9135 - val_loss: 0.2755 - val_accuracy: 0.8961
Epoch 6/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.2049 - accuracy: 0.9185 - val_loss: 0.2764 - val_accuracy: 0.8961
Epoch 7/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.1965 - accuracy: 0.9233 - val_loss: 0.2797 - val_accuracy: 0.8950
Epoch 8/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.1918 - accuracy: 0.9248 - val_loss: 0.2848 - val_accuracy: 0.8907
Epoch 9/10
8308/8308 [==============================] - 0s 30us/sample - loss: 0.1863 - accuracy: 0.9251 - val_loss: 0.2912 - val_accuracy: 0.8874
Epoch 10/10
8308/8308 [==============================] - 0s 35us/sample - loss: 0.1771 - accuracy: 0.9298 - val_loss: 0.2931 - val_accuracy: 0.8896
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 65us/sample - loss: 0.3498 - accuracy: 0.8556 - val_loss: 0.2724 - val_accuracy: 0.8918
Epoch 2/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2390 - accuracy: 0.9043 - val_loss: 0.2690 - val_accuracy: 0.8961
Epoch 3/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2172 - accuracy: 0.9145 - val_loss: 0.2671 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.2039 - accuracy: 0.9167 - val_loss: 0.2684 - val_accuracy: 0.8929
Epoch 5/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1916 - accuracy: 0.9245 - val_loss: 0.2797 - val_accuracy: 0.8853
Epoch 6/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1795 - accuracy: 0.9301 - val_loss: 0.2823 - val_accuracy: 0.8907
Epoch 7/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1675 - accuracy: 0.9337 - val_loss: 0.2909 - val_accuracy: 0.8810
Epoch 8/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1514 - accuracy: 0.9432 - val_loss: 0.2949 - val_accuracy: 0.8885
Epoch 9/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1443 - accuracy: 0.9456 - val_loss: 0.3078 - val_accuracy: 0.8766
Epoch 10/10
8308/8308 [==============================] - 0s 29us/sample - loss: 0.1304 - accuracy: 0.9504 - val_loss: 0.3175 - val_accuracy: 0.8896
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 70us/sample - loss: 0.4551 - accuracy: 0.7980 - val_loss: 0.3700 - val_accuracy: 0.8496
Epoch 2/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.3396 - accuracy: 0.8631 - val_loss: 0.3417 - val_accuracy: 0.8658
Epoch 3/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.3050 - accuracy: 0.8792 - val_loss: 0.3337 - val_accuracy: 0.8669
Epoch 4/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.2803 - accuracy: 0.8896 - val_loss: 0.3304 - val_accuracy: 0.8658
Epoch 5/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.2664 - accuracy: 0.8916 - val_loss: 0.3317 - val_accuracy: 0.8636
Epoch 6/10
8308/8308 [==============================] - 0s 35us/sample - loss: 0.2542 - accuracy: 0.9002 - val_loss: 0.3264 - val_accuracy: 0.8604
Epoch 7/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.2472 - accuracy: 0.9048 - val_loss: 0.3309 - val_accuracy: 0.8658
Epoch 8/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.2378 - accuracy: 0.9083 - val_loss: 0.3275 - val_accuracy: 0.8636
Epoch 9/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.2297 - accuracy: 0.9117 - val_loss: 0.3301 - val_accuracy: 0.8712
Epoch 10/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.2243 - accuracy: 0.9123 - val_loss: 0.3308 - val_accuracy: 0.8723
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 102us/sample - loss: 0.3451 - accuracy: 0.8565 - val_loss: 0.2679 - val_accuracy: 0.8885
Epoch 2/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.2281 - accuracy: 0.9119 - val_loss: 0.2582 - val_accuracy: 0.8950
Epoch 3/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.2037 - accuracy: 0.9194 - val_loss: 0.2673 - val_accuracy: 0.8972
Epoch 4/10
8308/8308 [==============================] - 0s 31us/sample - loss: 0.1884 - accuracy: 0.9280 - val_loss: 0.2630 - val_accuracy: 0.8983
Epoch 5/10
8308/8308 [==============================] - 0s 31us/sample - loss: 0.1754 - accuracy: 0.9326 - val_loss: 0.2675 - val_accuracy: 0.8961
Epoch 6/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.1610 - accuracy: 0.9387 - val_loss: 0.2721 - val_accuracy: 0.8885
Epoch 7/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.1517 - accuracy: 0.9417 - val_loss: 0.2794 - val_accuracy: 0.8853
Epoch 8/10
8308/8308 [==============================] - 0s 31us/sample - loss: 0.1418 - accuracy: 0.9462 - val_loss: 0.2903 - val_accuracy: 0.8874
Epoch 9/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.3046 - val_accuracy: 0.8885
Epoch 10/10
8308/8308 [==============================] - 0s 32us/sample - loss: 0.1226 - accuracy: 0.9537 - val_loss: 0.3072 - val_accuracy: 0.8864
Found 1193514 word vectors.
Train on 8308 samples, validate on 924 samples
Epoch 1/10
8308/8308 [==============================] - 1s 73us/sample - loss: 0.3737 - accuracy: 0.8347 - val_loss: 0.2676 - val_accuracy: 0.8874
Epoch 2/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.2266 - accuracy: 0.9073 - val_loss: 0.2643 - val_accuracy: 0.8961
Epoch 3/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.2003 - accuracy: 0.9208 - val_loss: 0.2651 - val_accuracy: 0.8939
Epoch 4/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.1828 - accuracy: 0.9284 - val_loss: 0.2738 - val_accuracy: 0.8907
Epoch 5/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.1635 - accuracy: 0.9351 - val_loss: 0.2816 - val_accuracy: 0.8950
Epoch 6/10
8308/8308 [==============================] - 0s 33us/sample - loss: 0.1476 - accuracy: 0.9411 - val_loss: 0.2804 - val_accuracy: 0.8853
Epoch 7/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.1298 - accuracy: 0.9521 - val_loss: 0.2901 - val_accuracy: 0.8950
Epoch 8/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.1121 - accuracy: 0.9565 - val_loss: 0.3008 - val_accuracy: 0.8950
Epoch 9/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.0964 - accuracy: 0.9652 - val_loss: 0.3358 - val_accuracy: 0.8788
Epoch 10/10
8308/8308 [==============================] - 0s 34us/sample - loss: 0.0835 - accuracy: 0.9703 - val_loss: 0.3472 - val_accuracy: 0.8874
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">sensitivity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.00</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="mf">0.50</span><span class="p">,</span> <span class="n">especificity</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">2.25</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;25&#39;</span><span class="p">,</span><span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">])</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Especificity&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Embedding dimension&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;32 cells&#39;</span><span class="p">,</span><span class="s1">&#39;64 cells&#39;</span><span class="p">,</span><span class="s1">&#39;128 cells&#39;</span><span class="p">],</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best accuracy= </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_52_0.png" src="../_images/U5.06 - [TALLER] - Padding, Masking - Sentiment Analysis-SOLUTION_52_0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best accuracy= 0.9047206582936336
</pre></div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ra√∫l Ramos, Juli√°n Arias / Universidad de Antioquia<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-43235448-3', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>