# Información 20211 - UdeA

## Trabajando con los materiales del curso

**01 - Jupyter notebooks y Google colab**: [video 13mins](https://youtu.be/KajSbrEBZ5k) Explicamos brevemente cómo es el entorno de ejecución de código Python en la nube de Google que vamos a usar durante el curso

**02 - Laboratorios y envío de soluciones**: [video 12mins](https://youtu.be/D6MuCnXc5LM) Describimos cómo es el mecanismo de envío de soluciones y la plataforma de autocorrección de talleres.


## Calendario y fechas

### Dedicación de tiempo sugerida

        19/feb - 21/mar: Unidades 1 y 2
        22/mar -  4/abr: Unidad 3
         5/abr -  2/may: Unidad 4
         3/may - 30/may: Unidad 5
        31/may - 14/jun: Proyecto

### Fechas límite para entregas

        21/mar: Laboratorios unidad 1 y 2
        28/mar: Proyecto entrega 1
        11/abr: Laboratorios unidad 3        
         2/may: Laboratorios unidad 4
        31/may: Laboratorios unidad 5
        14/jun: Proyecto informe final


## Evaluaciones

        20%     Laboratorios unidades 1,2,3
        20%     Laboratorios unidad 4
        20%     Laboratorios unidad 5
        40%     Proyecto 
        
## Proyecto

Deberás de realizar un proyecto que aplique las técnicas del módulo 4 o del módulo 5 a un problema que escojas. Por ejemplo:

- Relacionado con tu trabajo de tesis
- Un challenge de www.kaggle.com (aunque ya haya pasado la competición)
- Sobre algún dataset público
- etc.

Te recomendamos que:

- Verifiques que los datos están disponibles antes de escoger tu proyecto.
- Estimes los requerimientos computacionales para generar los modelos que necesites. Reduce el alcance de tu proyecto si lo necesitas (menos datos, menos clases, etc.).
- Realices una primera iteración cuanto antes. Es decir, que llegues a tener un primer modelo **sencillo** produciendo predicciones. Implementa en esta primera iteración estrictamente lo que necesites para tener un modelo. El objetivo es resolver la mayoría de los problemas técnicos que te puedan surgir para ya, después, enfocarte en todo lo que quieras hacer en las siguientes iteraciones (preprocesado de datos, otros modelos, etc.)

### Entrega 1

Tendars que entregar un documento (1-3 páginas máximo) con la siguiente estructure
- Contexto de aplicación 
- Objetivo de machine learning (queremos predecir X, dada tal información)
- Dataset: tipo de datos, tamaño (número de datos y tamaño en disco), distribución de las clases
- Métricas de desempeño (de machine learning y negocio)
- Referencias y resultados previos

### Entrega 2

- Notebooks reproducibles
- Informe


### Evaluación

        5%: Entrega 1 realizada a tiempo
       15%: claridad
       10%: reproducibilidad
       10%: compleción


## Referencias

**Guías sobre Deep Learning**

Aggarwal, Charu C. "Neural networks and deep learning." Springer 10 (2018): 978-3.

Calin, Ovidiu. Deep Learning Architectures. Springer International Publishing, 2020.

Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT Press [website](https://www.deeplearningbook.org/) [pdf](https://github.com/janishar/mit-deep-learning-book-pdf)


**Con un foco especial en alguno de los módulos**

- Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.

- Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).

- Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., & Saenko, K. (2015). Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision (pp. 4534-4542).

- Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112).

- Pradeep Pujari, Md. Rezaul Karim, Mohit Sewak (2017) , Practical Convolutional Neural Networks, O’Reilly, 

**Generales de machine learning**

- Hastie, Tibshirani, Friedman, **The Elements of Statistical Learning**, Springer-Verlag [website](https://web.stanford.edu/~hastie/ElemStatLearn/) [pdf](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)

- Haykin, S. S., Haykin, S. S., Haykin, S. S., & Haykin, S. S. (2009). Neural networks and learning machines (Vol. 3). Upper Saddle River, NJ, USA:: Pearson.

**Materiales complementarios**

- Curso de pregrado **Modelos y simulación**: [https://github.com/jdariasl/ML_2020](https://github.com/jdariasl/ML_2020)

- Curso de Pregrado **Introducción a la IA para las Ciecnias e Ingenierías** [https://rramosp.github.io/ai4eng.v1](https://rramosp.github.io/ai4eng.v1)


- [https://numpy.org/doc/stable/user/numpy-for-matlab-users.html](https://numpy.org/doc/stable/user/numpy-for-matlab-users.html)



## Contacto

[raul.ramos@udea.edu.co](mailto:raul.ramos@udea.edu.co), [julian.ariasl@udea.edu.co](mailto:julian.ariasl@udea.edu.co)
